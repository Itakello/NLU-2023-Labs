{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Constituency Grammars with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "- Understanding: \n",
    "    - relation between grammar and syntactic parse tree\n",
    "    - relation between grammar and syntactic categories\n",
    "    - relation between grammar and Part-of-Speech tags\n",
    "    - context free grammars (CFG)\n",
    "    - probabilistic context free grammars (PCFG)\n",
    "- Learning how to:\n",
    "    - define CFG in NLTK\n",
    "    - parse with CFG\n",
    "    - learn PCFGs from a treebank\n",
    "    - parse with PCFG\n",
    "    - generate sentences using a grammar in NLTK\n",
    "    - evaluate parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommended Reading\n",
    "- Dan Jurafsky and James H. Martin. [__Speech and Language Processing__ (SLP)](https://web.stanford.edu/~jurafsky/slp3/) (3rd ed. draft)\n",
    "- Steven Bird, Ewan Klein, and Edward Loper. [__Natural Language Processing with Python__ (NLTK)](https://www.nltk.org/book/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covered Material\n",
    "- SLP\n",
    "    - [Chapter 12: Constituency Grammars](https://web.stanford.edu/~jurafsky/slp3/12.pdf)\n",
    "    - [Chapter 13: Constituency Parsing](https://web.stanford.edu/~jurafsky/slp3/13.pdf)\n",
    "- NLTK \n",
    "    - [Chapter 8: Analyzing Sentence Structure](https://www.nltk.org/book/ch08.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Requirements\n",
    "\n",
    "- [NLTK](https://www.nltk.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Grammars, Production Rules, and Parse Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In linguistics, [**syntax**](https://en.wikipedia.org/wiki/Syntax) is the study of how words and morphemes combine to form larger units such as phrases and sentences. Central concerns of syntax include word order, **grammatical relations**, **hierarchical sentence structure** (constituency), agreement, the nature of crosslinguistic variation, and the relationship between form and meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In linguistics, the [**grammar**](https://en.wikipedia.org/wiki/Grammar) of a natural language is its set of structural constraints on speakers' or writers' composition of clauses, phrases, and words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [**context-free grammar (CFG)**](https://en.wikipedia.org/wiki/Context-free_grammar) is a formal grammar whose [**production rules**](https://en.wikipedia.org/wiki/Production_(computer_science)) are of the form:\n",
    "\n",
    "$$A \\to \\alpha$$\n",
    "\n",
    "where: \n",
    "- $A$ a single nonterminal symbol\n",
    "- $\\alpha$  a string of terminals and/or nonterminals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Parsing**](https://en.wikipedia.org/wiki/Parsing), syntax analysis, or syntactic analysis is the process of analyzing a string of symbols, either in natural language, computer languages or data structures, conforming to the rules of a formal grammar. \n",
    "Within computational linguistics the term is used to refer to the formal analysis by a computer of a sentence or other string of words into its **constituents**, resulting in a **parse tree** showing their syntactic relation to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [**parse tree**](https://en.wikipedia.org/wiki/Parse_tree) or **parsing tree** or **derivation tree** or **concrete syntax tree** is an ordered, rooted tree that represents the syntactic structure of a string according to some **context-free grammar**. The term parse tree itself is used primarily in computational linguistics; in theoretical syntax, the term syntax tree is more common."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Parse Tree Representation\n",
    "One of the possible parse trees for a sentence `I saw the man with a telescope` is as below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "      S                                    \n",
    "  ____|___________                          \n",
    " |                VP                       \n",
    " |     ___________|________                 \n",
    " |    |       |            PP              \n",
    " |    |       |        ____|___             \n",
    " NP   |       NP      |        NP          \n",
    " |    |    ___|___    |     ___|______      \n",
    "PRON  V  Det      N   P   Det         N    \n",
    " |    |   |       |   |    |          |     \n",
    " I   saw the     man with  a      telescope\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To occupy less space, a parse tree is usually represented using \"bracketed expressions\", where brackets enclose each **constituent** and the first element of the expression is the tree-node label.\n",
    "\n",
    "```\n",
    "(S\n",
    "  (NP (PRON I))\n",
    "  (VP\n",
    "    (V saw)\n",
    "    (NP (Det the) (N man))\n",
    "    (PP (P with) (NP (Det a) (N telescope)))))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is common to write a parse tree in a less readable one-line expression:\n",
    "\n",
    "```\n",
    "(S (NP (PRON I)) (VP (V saw) (NP (Det the) (N man)) (PP (P with) (NP (Det a) (N telescope)))))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Parse Trees in NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK provides `Tree` class for representing hierarchical language structures. The class implements many useful methods. <br> Full documentation [HERE](https://www.nltk.org/api/nltk.tree.tree.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1. \"Core\" Methods\n",
    "\n",
    "- `fromstring()` reads a bracketed tree string and return the resulting tree\n",
    "\n",
    "- `productions()` generate the productions that correspond to the non-terminal nodes of the tree.\n",
    "    - For each subtree of the form `(P: C1 C2 ... Cn)` this produces a production of the form `P -> C1 C2 ... Cn`.\n",
    "\n",
    "- `label()` returns the node label of the tree\n",
    "\n",
    "- `subtrees()` generates all the subtrees of this tree\n",
    "\n",
    "- `pos()` return a sequence of pos-tagged words extracted from the tree.\n",
    "\n",
    "- `leaves()` returns the leaves of the tree.\n",
    "\n",
    "- `flatten()` return a flat version of the tree, with all non-root non-terminals removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tree import Tree\n",
    "sent = \"I saw the man with a telescope\"\n",
    "parse_tree_str = \"(S (NP (PRON I)) (VP (V saw) (NP (Det the) (N man)) (PP (P with) (NP (Det a) (N telescope)))))\"\n",
    "\n",
    "tree = Tree.fromstring(parse_tree_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP (PRON I))\n",
      "  (VP\n",
      "    (V saw)\n",
      "    (NP (Det the) (N man))\n",
      "    (PP (P with) (NP (Det a) (N telescope)))))\n"
     ]
    }
   ],
   "source": [
    "# prints bracketed expression for the parse tree\n",
    "print(tree) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "(S\n",
      "  (NP (PRON I))\n",
      "  (VP\n",
      "    (V saw)\n",
      "    (NP (Det the) (N man))\n",
      "    (PP (P with) (NP (Det a) (N telescope)))))\n",
      "-----------------------------------------------------------------------------------------\n",
      "(NP (PRON I))\n",
      "-----------------------------------------------------------------------------------------\n",
      "(PRON I)\n",
      "-----------------------------------------------------------------------------------------\n",
      "(VP\n",
      "  (V saw)\n",
      "  (NP (Det the) (N man))\n",
      "  (PP (P with) (NP (Det a) (N telescope))))\n",
      "-----------------------------------------------------------------------------------------\n",
      "(V saw)\n",
      "-----------------------------------------------------------------------------------------\n",
      "(NP (Det the) (N man))\n",
      "-----------------------------------------------------------------------------------------\n",
      "(Det the)\n",
      "-----------------------------------------------------------------------------------------\n",
      "(N man)\n",
      "-----------------------------------------------------------------------------------------\n",
      "(PP (P with) (NP (Det a) (N telescope)))\n",
      "-----------------------------------------------------------------------------------------\n",
      "(P with)\n",
      "-----------------------------------------------------------------------------------------\n",
      "(NP (Det a) (N telescope))\n",
      "-----------------------------------------------------------------------------------------\n",
      "(Det a)\n",
      "-----------------------------------------------------------------------------------------\n",
      "(N telescope)\n"
     ]
    }
   ],
   "source": [
    "# prints all the subtrees\n",
    "for stree in tree.subtrees():\n",
    "    print('-'*89)\n",
    "    print(stree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Visualizaing Parse Trees\n",
    "\n",
    "In the code above, we have printed the parse tree using `print()` method, which prints a _bracketed expression_ tree.\n",
    "It is also possible to visualize syntactic trees of using other method: \n",
    "\n",
    "- `pprint()` the same as above\n",
    "- `pretty_print()` draws ASCII tree (the original example)\n",
    "- `draw()` opens a new window containing a graphical diagram of this tree.\n",
    "- `tree` a call to a tree draws it using `svgling` \n",
    "    - requires `svgling` module to be installed (`pip install svgling`)\n",
    "    - GitHub https://github.com/rawlins/svgling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         S                                                         \n",
      " ┌───────┴────────────────────┐                                        \n",
      " │                            VP                                   \n",
      " │       ┌─────────────┬──────┴──────────────┐                         \n",
      " │       │             │                     PP                    \n",
      " │       │             │             ┌───────┴──────┐                  \n",
      " NP      │             NP            │              NP             \n",
      " │       │      ┌──────┴──────┐      │       ┌──────┴─────────┐        \n",
      "PRON     V     Det            N      P      Det               N    \n",
      " │       │      │             │      │       │                │        \n",
      " I      saw    the           man    with     a            telescope\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "tree\n",
    "print(tree.pretty_print(unicodelines=True, nodedist=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP (PRON I))\n",
      "  (VP\n",
      "    (V saw)\n",
      "    (NP (Det the) (N man))\n",
      "    (PP (P with) (NP (Det a) (N telescope)))))\n",
      "None\n",
      "      S                                    \n",
      "  ____|___________                          \n",
      " |                VP                       \n",
      " |     ___________|________                 \n",
      " |    |       |            PP              \n",
      " |    |       |        ____|___             \n",
      " NP   |       NP      |        NP          \n",
      " |    |    ___|___    |     ___|______      \n",
      "PRON  V  Det      N   P   Det         N    \n",
      " |    |   |       |   |    |          |     \n",
      " I   saw the     man with  a      telescope\n",
      "\n",
      "None\n",
      "         S                                                         \n",
      " ┌───────┴────────────────────┐                                        \n",
      " │                            VP                                   \n",
      " │       ┌─────────────┬──────┴──────────────┐                         \n",
      " │       │             │                     PP                    \n",
      " │       │             │             ┌───────┴──────┐                  \n",
      " NP      │             NP            │              NP             \n",
      " │       │      ┌──────┴──────┐      │       ┌──────┴─────────┐        \n",
      "PRON     V     Det            N      P      Det               N    \n",
      " │       │      │             │      │       │                │        \n",
      " I      saw    the           man    with     a            telescope\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "print(tree.pprint())\n",
    "print(tree.pretty_print()) # or\n",
    "print(tree.pretty_print(unicodelines=True, nodedist=4)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "Familiarize with the [`Tree`](https://www.nltk.org/api/nltk.tree.tree.html) class.\n",
    "\n",
    "- Consult the documentation for more detail (and other methods)\n",
    "- Try each of the \"core\" methods listed above\n",
    "    - see production rules, leaves, and pos\n",
    "\n",
    "######   \"Core\" Methods\n",
    "\n",
    "- `fromstring()` reads a bracketed tree string and return the resulting tree\n",
    "\n",
    "- `productions()` generate the productions that correspond to the non-terminal nodes of the tree.\n",
    "    - For each subtree of the form `(P: C1 C2 ... Cn)` this produces a production of the form `P -> C1 C2 ... Cn`.\n",
    "\n",
    "- `label()` returns the node label of the tree\n",
    "\n",
    "- `subtrees()` generates all the subtrees of this tree\n",
    "\n",
    "- `pos()` return a sequence of pos-tagged words extracted from the tree.\n",
    "\n",
    "- `leaves()` returns the leaves of the tree.\n",
    "\n",
    "- `flatten()` return a flat version of the tree, with all non-root non-terminals removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Productions: [S -> NP VP, NP -> PRON, PRON -> 'I', VP -> V NP PP, V -> 'saw', NP -> Det N, Det -> 'the', N -> 'man', PP -> P NP, P -> 'with', NP -> Det N, Det -> 'a', N -> 'telescope']\n",
      "Label: S\n",
      "Pos Tags: [('I', 'PRON'), ('saw', 'V'), ('the', 'Det'), ('man', 'N'), ('with', 'P'), ('a', 'Det'), ('telescope', 'N')]\n",
      "Leaves: ['I', 'saw', 'the', 'man', 'with', 'a', 'telescope']\n",
      "Flatten: (S I saw the man with a telescope)\n"
     ]
    }
   ],
   "source": [
    "parse_tree_str = \"(S (NP (PRON I)) (VP (V saw) (NP (Det the) (N man)) (PP (P with) (NP (Det a) (N telescope)))))\"\n",
    "tree = Tree.fromstring(parse_tree_str)\n",
    "\n",
    "# Productions\n",
    "print('Productions:', tree.productions())\n",
    "\n",
    "# Label\n",
    "print('Label:', tree.label())\n",
    "\n",
    "# Pos tags\n",
    "print('Pos Tags:', tree.pos())\n",
    "\n",
    "# Leaves \n",
    "print('Leaves:', tree.leaves())\n",
    "\n",
    "# Flatten\n",
    "print('Flatten:', tree.flatten())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Context Free Grammars (CFG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.1. Defining Context Free Grammars\n",
    "\n",
    "CFG are defined by a *start symbol* and a set of *production rules*. The *start symbol* defines the root node of parse trees (usually __S__). \n",
    "\n",
    "*Production rules* specify allowed parent-child relations in a parse tree. Each production specifies what node can be the parent of a particular set of children nodes. \n",
    "\n",
    "For example, the production `S -> NP VP` specifies that an `S` node can be the parent of an `NP` node and a `VP` node.\n",
    "\n",
    "The left-hand side of a production rules specifies potential *non-terminal* parent nodes; while right-hand side specifies list of allowed *non-terminal* and *terminal* (text) children. \n",
    "\n",
    "A production like `VP -> V NP | VP PP` has a disjunction on the right-hand side, shown by the `|` and is an abbreviation for the two productions `VP -> V NP` and `VP -> V NP PP`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 2.1.1 Syntactic Categories\n",
    "\n",
    "| __Symbol__ | __Meaning__ | __Example__ |\n",
    "|:-----------|:------------|:------------|\n",
    "| S   | sentence             | I saw the man |\n",
    "| NP  | noun phrase          | the man | \n",
    "| VP  | verb phrase          | saw the man |\n",
    "| PP  | prepositional phrase | with a telescope |\n",
    "| Det | determiner  | the |\n",
    "| N   | noun        | man |\n",
    "| V   | verb        | saw |\n",
    "| P   | preposition | with |\n",
    "\n",
    "\n",
    "- Non-Terminals: `S`, `NP`, `VP`, `PP`\n",
    "- Pre-Terminals: `Det`, `N`, `V`, `P` (Part-of-Speech Tags)\n",
    "- Terminals (Leaves): the, man, saw, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.2. Defining Context Free Grammars in NLTK\n",
    "\n",
    "The grammar can be defined as a string or as a list of strings of production rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar with 14 productions (start state = S)\n",
      "    S -> NP VP\n",
      "    NP -> Det N\n",
      "    NP -> Det N PP\n",
      "    NP -> PRON\n",
      "    VP -> V NP\n",
      "    VP -> V NP PP\n",
      "    PP -> P NP\n",
      "    Det -> 'the'\n",
      "    Det -> 'a'\n",
      "    N -> 'man'\n",
      "    N -> 'telescope'\n",
      "    PRON -> 'I'\n",
      "    V -> 'saw'\n",
      "    P -> 'with'\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "rules = [\n",
    "    'S -> NP VP',\n",
    "    'NP -> Det N | Det N PP | PRON',\n",
    "    'VP -> V NP | V NP PP',\n",
    "    'PP -> P NP',\n",
    "    'Det -> \"the\" | \"a\"',\n",
    "    'N -> \"man\" | \"telescope\"',\n",
    "    'PRON -> \"I\"',\n",
    "    'V -> \"saw\"',\n",
    "    'P -> \"with\"'   \n",
    "]\n",
    "\n",
    "toy_grammar = nltk.CFG.fromstring(rules)\n",
    "\n",
    "print(toy_grammar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Grammar object has 2 components:\n",
    "- start symbol\n",
    "- production rules\n",
    "\n",
    "Those can be access as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S\n"
     ]
    }
   ],
   "source": [
    "print(toy_grammar.start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[S -> NP VP, NP -> Det N, NP -> Det N PP, NP -> PRON, VP -> V NP, VP -> V NP PP, PP -> P NP, Det -> 'the', Det -> 'a', N -> 'man', N -> 'telescope', PRON -> 'I', V -> 'saw', P -> 'with']\n"
     ]
    }
   ],
   "source": [
    "print(toy_grammar.productions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Each production has 2 parts:\n",
    "- left-hand side\n",
    "- right-hand side\n",
    "\n",
    "which can be accessed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S\n",
      "(NP, VP)\n"
     ]
    }
   ],
   "source": [
    "rule = toy_grammar.productions()[0]\n",
    "print(rule.lhs())\n",
    "print(rule.rhs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Grammar__'s `productions(lhs=None, rhs=None, empty=False)` methos returns the grammar productions, filtered by the left-hand side or the first item in the right-hand side.\n",
    "\n",
    "__Parameters__\n",
    "- `lhs` -- Only return productions with the given left-hand side.\n",
    "- `rhs` -- Only return productions with the given first item in the right-hand side.\n",
    "- `empty` -- Only return productions with an empty right-hand side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NP -> Det N, NP -> Det N PP, NP -> PRON]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import Nonterminal\n",
    "toy_grammar.productions(lhs=Nonterminal('NP'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[S -> NP VP]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_grammar.productions(rhs=Nonterminal('NP'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_grammar.productions(empty=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.3. Parsing with CFG\n",
    "\n",
    "> A parser processes input sentences according to the productions of a grammar, and builds one or more constituent structures that conform to the grammar. A grammar is a declarative specification of well-formedness — it is actually just a string, not a program. A parser is a procedural interpretation of the grammar. It searches through the space of trees licensed by a grammar to find one that has the required sentence along its fringe (outer edges).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 2.3.1. Available CFG Parsers in NLTK\n",
    "\n",
    "- Recursive descent parsing\n",
    "    - top-down algorithm\n",
    "    - pro: finds all successful parses.\n",
    "    - con: inefficient. will try all rules brute-force, even the ones that do not match the input. Goes into an infinite loop when handling a left-recursive rule.\n",
    "    - `nltk.RecursiveDescentParser()`\n",
    "\n",
    "- Shift-reduce parsing\n",
    "    - bottom-up algorithm\n",
    "    - pro: efficient. only works with the rules that match input words.\n",
    "    - con: may fail to find a legitimate parse even when there is one.\n",
    "    - `nltk.ShiftReduceParser()`\n",
    "\n",
    "- The left-corner parser\n",
    "    - a top-down parser with bottom-up filtering\n",
    "    - `nltk.LeftCornerChartParser()`: combines left-corner parsing and chart parsing\n",
    "\n",
    "- Chart parsing\n",
    "    - utilizes dynamic programming: builds and refers to well-formed substring tables (WFST)\n",
    "    - pro: efficient.\n",
    "    - con: may take up a big memory space when dealing with a long sentence.\n",
    "    - `nltk.ChartParser()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = [\n",
    "    'S -> NP VP',\n",
    "    'NP -> Det N | Det N PP | PRON',\n",
    "    'VP -> V NP | V NP PP',\n",
    "    'PP -> P NP',\n",
    "    'Det -> \"the\" | \"a\"',\n",
    "    'N -> \"man\" | \"telescope\"',\n",
    "    'PRON -> \"I\"',\n",
    "    'V -> \"saw\"',\n",
    "    'P -> \"with\"'   \n",
    "]\n",
    "\n",
    "toy_grammar = nltk.CFG.fromstring(rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP (PRON I))\n",
      "  (VP\n",
      "    (V saw)\n",
      "    (NP (Det the) (N man))\n",
      "    (PP (P with) (NP (Det a) (N telescope)))))\n",
      "(S\n",
      "  (NP (PRON I))\n",
      "  (VP\n",
      "    (V saw)\n",
      "    (NP (Det the) (N man) (PP (P with) (NP (Det a) (N telescope))))))\n"
     ]
    }
   ],
   "source": [
    "parser = nltk.ChartParser(toy_grammar)\n",
    "\n",
    "sent = \"I saw the man with a telescope\"\n",
    "\n",
    "for tree in parser.parse(sent.split()):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The sentence produces two possible parse trees. Thus, it is said to be structurally ambiguous -- prepositional phrase attachment ambiguity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise\n",
    "\n",
    "- Define grammar that covers the following sentences.\n",
    "\n",
    "    - show flights from new york to los angeles\n",
    "    - list flights from new york to los angeles\n",
    "    - show flights from new york\n",
    "    - list flights to los angeles\n",
    "    - list flights\n",
    "- Use one of the parsers to parse the sentences (i.e. test your grammar)\n",
    "\n",
    "**Note:** <br>\n",
    "- start from Verb Pharse\n",
    "- Prepositional Phrase (PP) are composed of a preposition and a Noun Phrase \n",
    "- 'los' and 'angeles' are two consecutive nouns (N) (same for new york)\n",
    "\n",
    "#### Useful reminder from above\n",
    "\n",
    "\n",
    "| __Symbol__ | __Meaning__ | __Example__ |\n",
    "|:-----------|:------------|:------------|\n",
    "| S   | sentence             | I saw the man |\n",
    "| NP  | noun phrase          | the man | \n",
    "| VP  | verb phrase          | saw the man |\n",
    "| PP  | prepositional phrase | with a telescope |\n",
    "| Det | determiner  | the |\n",
    "| N   | noun        | man |\n",
    "| V   | verb        | saw |\n",
    "| P   | preposition | with |\n",
    "\n",
    "\n",
    "- Non-Terminals: `S`, `NP`, `VP`, `PP`\n",
    "- Pre-Terminals: `Det`, `N`, `V`, `P` (Part-of-Speech Tags)\n",
    "- Terminals (Leaves): the, man, saw, ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# test setenteces\n",
    "test_sents = [\n",
    "    \"show flights from new york to los angeles\", \n",
    "    \"list flights from new york to los angeles\",\n",
    "    \"show flights from new york\",\n",
    "    \"list flights to los angeles\",\n",
    "    \"list flights\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = [\n",
    "    'S -> VP | VP PP | VP PP PP',\n",
    "    'VP -> V NP',\n",
    "    'NP -> N | N N',\n",
    "    'PP -> P NP',\n",
    "    'P -> \"from\" | \"to\"',\n",
    "    'N -> \"los\" | \"angeles\" | \"new\" | \"york\" | \"flights\"' ,\n",
    "    'V -> \"show\" | \"list\"' \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "show flights from new york to los angeles\n",
      "(S\n",
      "  (VP (V show) (NP (N flights)))\n",
      "  (PP (P from) (NP (N new) (N york)))\n",
      "  (PP (P to) (NP (N los) (N angeles))))\n",
      "None\n",
      "-----------------------------------------------------------------------------------------\n",
      "list flights from new york to los angeles\n",
      "(S\n",
      "  (VP (V list) (NP (N flights)))\n",
      "  (PP (P from) (NP (N new) (N york)))\n",
      "  (PP (P to) (NP (N los) (N angeles))))\n",
      "None\n",
      "-----------------------------------------------------------------------------------------\n",
      "show flights from new york\n",
      "(S\n",
      "  (VP (V show) (NP (N flights)))\n",
      "  (PP (P from) (NP (N new) (N york))))\n",
      "None\n",
      "-----------------------------------------------------------------------------------------\n",
      "list flights to los angeles\n",
      "(S\n",
      "  (VP (V list) (NP (N flights)))\n",
      "  (PP (P to) (NP (N los) (N angeles))))\n",
      "None\n",
      "-----------------------------------------------------------------------------------------\n",
      "list flights\n",
      "(S (VP (V list) (NP (N flights))))\n",
      "None\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "toy_grammar = nltk.CFG.fromstring(rules)\n",
    "parser = nltk.ChartParser(toy_grammar)\n",
    "for sent in test_sents:\n",
    "    print(sent)\n",
    "    for tree in parser.parse(sent.split()):\n",
    "        print(tree.pprint())\n",
    "    print('-'*89)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2. \"Real\" Grammars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to load a grammar written by someone else into NLTK.\n",
    "\n",
    "- run `nltk.download()`\n",
    "- go to `Models` tab\n",
    "- download `large_grammars`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package large_grammars to\n",
      "[nltk_data]     C:\\Users\\maxst\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping grammars\\large_grammars.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('large_grammars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "atis_grammar = nltk.data.load('grammars/large_grammars/atis.cfg')\n",
    "atis_parser = nltk.ChartParser(atis_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Grammar with 5517 productions>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atis_grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grammar comes with some test sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['i', 'need', 'a', 'flight', 'from', 'charlotte', 'to', 'las', 'vegas', 'that', 'makes', 'a', 'stop', 'in', 'saint', 'louis', '.'], 2085)\n"
     ]
    }
   ],
   "source": [
    "atis_test_sentences = nltk.data.load('grammars/large_grammars/atis_sentences.txt')\n",
    "atis_test_sentences = nltk.parse.util.extract_test_sentences(atis_test_sentences)\n",
    "print(atis_test_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each test sentence is a tuple of a list of sentence words and a number of possible parses with respect to the grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2085 2085\n",
      "1380 1380\n",
      "50 50\n"
     ]
    }
   ],
   "source": [
    "# let's check the number of parses our parser produces\n",
    "for sent, pnum in atis_test_sentences[:3]:\n",
    "    parses = atis_parser.parse(sent)\n",
    "    print(len(list(parses)), pnum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "Parse the sentences from the excercise above using **ATIS_GRAMMAR**\n",
    "- try different parsers\n",
    "- output the number of parses each sentence yields "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "atis_parser = nltk.ChartParser(atis_grammar)\n",
    "for sent in test_sents:\n",
    "    parses = atis_parser.parse(sent.split())\n",
    "    print(len(list(parses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Probabilistic Context Free Grammars (PCFG)\n",
    "\n",
    "PCFGs are very similar to CFGs - they just have an additional probability for each production. \n",
    "\n",
    "For a given left-hand-side non-terminal, the sum of the probabilities must be 1.0!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar with 14 productions (start state = S)\n",
      "    S -> NP VP [1.0]\n",
      "    NP -> Det N [0.6]\n",
      "    NP -> Det N PP [0.3]\n",
      "    NP -> PRON [0.1]\n",
      "    VP -> V NP [0.7]\n",
      "    VP -> V NP PP [0.3]\n",
      "    PP -> P NP [1.0]\n",
      "    Det -> 'the' [0.5]\n",
      "    Det -> 'a' [0.5]\n",
      "    N -> 'man' [0.5]\n",
      "    N -> 'telescope' [0.5]\n",
      "    PRON -> 'I' [1.0]\n",
      "    V -> 'saw' [1.0]\n",
      "    P -> 'with' [1.0]\n"
     ]
    }
   ],
   "source": [
    "weighted_rules = [\n",
    "    'S -> NP VP [1.0]',\n",
    "    'NP -> Det N [0.6]',\n",
    "    'NP -> Det N PP [0.3]',\n",
    "    'NP -> PRON [0.1]',\n",
    "    'VP -> V NP [0.7]',\n",
    "    'VP -> V NP PP [0.3]',\n",
    "    'PP -> P NP [1.0]',\n",
    "    'Det -> \"the\" [0.5]',\n",
    "    'Det -> \"a\" [0.5]',\n",
    "    'N -> \"man\" [0.5]',\n",
    "    'N -> \"telescope\" [0.5]',\n",
    "    'PRON -> \"I\" [1.0]',\n",
    "    'V -> \"saw\" [1.0]',\n",
    "    'P -> \"with\" [1.0]'   \n",
    "]\n",
    "\n",
    "toy_grammar = nltk.PCFG.fromstring(weighted_rules)\n",
    "\n",
    "print(toy_grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "On top of right-hand side and left-hand side, probabilistic rules have probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S\n",
      "(NP, VP)\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "rule = toy_grammar.productions()[0]\n",
    "print(rule.lhs())\n",
    "print(rule.rhs())\n",
    "print(rule.prob())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.1. Learning Grammars from a Treebank\n",
    "\n",
    "The most important method consists of inducing a PCFG from trees in a treebank (`induce_pcfg()`). \n",
    "\n",
    "NLTK provides portion of Penn Treebank corpus, which we can utilize to induce rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\maxst\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Production rules can be extracted using `productions()` method iterating over parsed sentences in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BracketParseCorpusReader in '.../corpora/treebank/combined' (not loaded yet)>\n",
      "179360\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "\n",
    "print(treebank)\n",
    "\n",
    "productions = []\n",
    "# let's keep it small\n",
    "for item in treebank.fileids():\n",
    "    for tree in treebank.parsed_sents(item):\n",
    "        productions += tree.productions()\n",
    "    \n",
    "print(len(productions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The grammar can be induced as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import Nonterminal\n",
    "S = Nonterminal('S')\n",
    "grammar = nltk.induce_pcfg(S, productions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Grammar with 21763 productions>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.2. PCFG Parsers in NLTK\n",
    "NLTK provides several PCFG parsers:\n",
    "\n",
    "From `nltk.parse.viterbi`\n",
    "\n",
    "- ViterbiParser\n",
    "    - A bottom-up PCFG parser that uses dynamic programming to find the single most likely parse for a text. The ViterbiParser parser parses texts by filling in a “most likely constituent table”. This table records the most probable tree representation for any given span and node value. In particular, it has an entry for every start index, end index, and node value, recording the most likely subtree that spans from the start index to the end index, and has the given node value.\n",
    "\n",
    "From `nltk.parse.pchart` module\n",
    "\n",
    "- InsideChartParser\n",
    "    - A bottom-up parser for PCFG grammars that tries edges in descending order of the inside probabilities of their trees.\n",
    "    - use `beam_size = len(tokens)+1` argument\n",
    "    \n",
    "- RandomChartParser\n",
    "    - A bottom-up parser for PCFG grammars that tries edges in random order. This sorting order results in a random search strategy.\n",
    "    \n",
    "- UnsortedChartParser\n",
    "    - A bottom-up parser for PCFG grammars that tries edges in whatever order.\n",
    "\n",
    "- LongestChartParser\n",
    "    - A bottom-up parser for PCFG grammars that tries longer edges before shorter ones. This sorting order results in a type of best-first search strategy.\n",
    "\n",
    "Read about them in the [documentation](http://www.nltk.org/api/nltk.parse.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's parse one of the sentences from above using Viterbi parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP-SBJ (NNP Show))\n",
      "  (NP-PRD\n",
      "    (NP (NP (PRP me)) (NNS flights))\n",
      "    (PP\n",
      "      (IN from)\n",
      "      (NP\n",
      "        (NP (NNP New) (NNP York))\n",
      "        (PP (TO to) (NP (NNP Los) (NNP Angeles))))))) (p=1.23346e-35)\n"
     ]
    }
   ],
   "source": [
    "parser = nltk.ViterbiParser(grammar)\n",
    "for tree in parser.parse(\"Show me flights from New York to Los Angeles\".split()):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise\n",
    "\n",
    "- Try different parser to parse the sentences from the exercises above\n",
    "- Compare assigned probabilities\n",
    "- Compare time it takes to parse sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentence = \"show me flights from New York to Los Angeles\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP-SBJ (NN show))\n",
      "  (NP-PRD\n",
      "    (NP (NP (PRP me)) (NNS flights))\n",
      "    (PP\n",
      "      (IN from)\n",
      "      (NP\n",
      "        (NP (NNP New) (NNP York))\n",
      "        (PP (TO to) (NP (NNP Los) (NNP Angeles))))))) (p=2.0478e-35)\n",
      "CPU times: total: 8.34 s\n",
      "Wall time: 10.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from nltk import ViterbiParser\n",
    "# Parser 1\n",
    "parser = ViterbiParser(grammar)\n",
    "for tree in parser.parse(tokenized_sentence):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[138], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#%%time\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# Parser 2\u001b[39;00m\n\u001b[0;32m      3\u001b[0m parser \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mRandomChartParser(grammar)\n\u001b[1;32m----> 4\u001b[0m \u001b[39mfor\u001b[39;00m tree \u001b[39min\u001b[39;00m parser\u001b[39m.\u001b[39;49mparse(tokenized_sentence):\n\u001b[0;32m      5\u001b[0m     \u001b[39mprint\u001b[39m(tree)\n",
      "File \u001b[1;32mc:\\Users\\maxst\\miniconda3\\envs\\NLU\\lib\\site-packages\\nltk\\parse\\pchart.py:260\u001b[0m, in \u001b[0;36mBottomUpProbabilisticChartParser.parse\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[39mprint\u001b[39m(\n\u001b[0;32m    255\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m  \u001b[39m\u001b[39m%-50s\u001b[39;00m\u001b[39m [\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    256\u001b[0m             \u001b[39m%\u001b[39m (chart\u001b[39m.\u001b[39mpretty_format_edge(edge, width\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m), edge\u001b[39m.\u001b[39mprob())\n\u001b[0;32m    257\u001b[0m         )\n\u001b[0;32m    259\u001b[0m     \u001b[39m# Apply BU & FR to it.\u001b[39;00m\n\u001b[1;32m--> 260\u001b[0m     queue\u001b[39m.\u001b[39;49mextend(bu\u001b[39m.\u001b[39;49mapply(chart, grammar, edge))\n\u001b[0;32m    261\u001b[0m     queue\u001b[39m.\u001b[39mextend(fr\u001b[39m.\u001b[39mapply(chart, grammar, edge))\n\u001b[0;32m    263\u001b[0m \u001b[39m# Get a list of complete parses.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\maxst\\miniconda3\\envs\\NLU\\lib\\site-packages\\nltk\\parse\\pchart.py:90\u001b[0m, in \u001b[0;36mProbabilisticBottomUpPredictRule.apply\u001b[1;34m(self, chart, grammar, edge)\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[39mfor\u001b[39;00m prod \u001b[39min\u001b[39;00m grammar\u001b[39m.\u001b[39mproductions():\n\u001b[1;32m---> 90\u001b[0m     \u001b[39mif\u001b[39;00m edge\u001b[39m.\u001b[39;49mlhs() \u001b[39m==\u001b[39;49m prod\u001b[39m.\u001b[39;49mrhs()[\u001b[39m0\u001b[39;49m]:\n\u001b[0;32m     91\u001b[0m         new_edge \u001b[39m=\u001b[39m ProbabilisticTreeEdge\u001b[39m.\u001b[39mfrom_production(\n\u001b[0;32m     92\u001b[0m             prod, edge\u001b[39m.\u001b[39mstart(), prod\u001b[39m.\u001b[39mprob()\n\u001b[0;32m     93\u001b[0m         )\n\u001b[0;32m     94\u001b[0m         \u001b[39mif\u001b[39;00m chart\u001b[39m.\u001b[39minsert(new_edge, ()):\n",
      "File \u001b[1;32mc:\\Users\\maxst\\miniconda3\\envs\\NLU\\lib\\site-packages\\nltk\\grammar.py:123\u001b[0m, in \u001b[0;36mNonterminal.__eq__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[39m    Return the node value corresponding to this ``Nonterminal``.\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \n\u001b[0;32m    119\u001b[0m \u001b[39m    :rtype: (any)\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_symbol\n\u001b[1;32m--> 123\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__eq__\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[0;32m    124\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[39m    Return True if this non-terminal is equal to ``other``.  In\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[39m    particular, return True if ``other`` is a ``Nonterminal``\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39m    :rtype: bool\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m==\u001b[39m \u001b[39mtype\u001b[39m(other) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_symbol \u001b[39m==\u001b[39m other\u001b[39m.\u001b[39m_symbol\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "# Parser 2\n",
    "parser = nltk.RandomChartParser(grammar)\n",
    "for tree in parser.parse(tokenized_sentence):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:3\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\maxst\\miniconda3\\envs\\NLU\\lib\\site-packages\\nltk\\parse\\pchart.py:245\u001b[0m, in \u001b[0;36mBottomUpProbabilisticChartParser.parse\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    241\u001b[0m     queue\u001b[39m.\u001b[39mappend(edge)\n\u001b[0;32m    243\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mlen\u001b[39m(queue) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    244\u001b[0m     \u001b[39m# Re-sort the queue.\u001b[39;00m\n\u001b[1;32m--> 245\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msort_queue(queue, chart)\n\u001b[0;32m    247\u001b[0m     \u001b[39m# Prune the queue to the correct size if a beam was defined\u001b[39;00m\n\u001b[0;32m    248\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeam_size:\n",
      "File \u001b[1;32mc:\\Users\\maxst\\miniconda3\\envs\\NLU\\lib\\site-packages\\nltk\\parse\\pchart.py:422\u001b[0m, in \u001b[0;36mLongestChartParser.sort_queue\u001b[1;34m(self, queue, chart)\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msort_queue\u001b[39m(\u001b[39mself\u001b[39m, queue, chart):\n\u001b[1;32m--> 422\u001b[0m     queue\u001b[39m.\u001b[39;49msort(key\u001b[39m=\u001b[39;49m\u001b[39mlambda\u001b[39;49;00m edge: edge\u001b[39m.\u001b[39;49mlength())\n",
      "File \u001b[1;32mc:\\Users\\maxst\\miniconda3\\envs\\NLU\\lib\\site-packages\\nltk\\parse\\pchart.py:422\u001b[0m, in \u001b[0;36mLongestChartParser.sort_queue.<locals>.<lambda>\u001b[1;34m(edge)\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msort_queue\u001b[39m(\u001b[39mself\u001b[39m, queue, chart):\n\u001b[1;32m--> 422\u001b[0m     queue\u001b[39m.\u001b[39msort(key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m edge: edge\u001b[39m.\u001b[39mlength())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Parser 3\n",
    "parser = nltk.LongestChartParser(grammar)\n",
    "for tree in parser.parse(tokenized_sentence):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Generating Sentences\n",
    "\n",
    "Grammars can be used to generate sentences as well. This is accomplished using `generate` method.\n",
    "read [here](http://www.nltk.org/api/nltk.parse.html#module-nltk.parse.generate)\n",
    "\n",
    "arguments it takes are the following `nltk.parse.generate.generate(grammar, start=None, depth=None, n=None)`:\n",
    "\n",
    "- grammar – The Grammar used to generate sentences.\n",
    "- start – The Nonterminal, which is and NLTK object, from which to start generate sentences.\n",
    "- depth – The maximal depth of the generated tree.\n",
    "- n – The maximum number of sentences to return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'man', 'saw', 'the', 'man']\n",
      "['the', 'man', 'saw', 'the', 'telescope']\n",
      "['the', 'man', 'saw', 'a', 'man']\n",
      "['the', 'man', 'saw', 'a', 'telescope']\n",
      "['the', 'man', 'saw', 'the', 'man', 'with', 'the', 'man']\n",
      "['the', 'man', 'saw', 'the', 'man', 'with', 'the', 'telescope']\n",
      "['the', 'man', 'saw', 'the', 'man', 'with', 'a', 'man']\n",
      "['the', 'man', 'saw', 'the', 'man', 'with', 'a', 'telescope']\n",
      "['the', 'man', 'saw', 'the', 'man', 'with', 'the', 'man', 'with', 'the', 'man']\n",
      "['the', 'man', 'saw', 'the', 'man', 'with', 'the', 'man', 'with', 'the', 'telescope']\n"
     ]
    }
   ],
   "source": [
    "from nltk.parse.generate import generate\n",
    "\n",
    "for sent in generate(toy_grammar, n=10):\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluating Constituency Parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Comparing Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since _constituency parser_ outputs _constituents_, parser evaluation is essentially a comparison between constituents of the reference and the automatic (hypothesis) parse trees (of the test set). Since constituents are labeled (have _syntactic categories_), the evaluation can also compare the reference and the hypothesis labels.\n",
    "\n",
    "A constituent in a hypothesis parse tree is considered correct, if there is a constituent in a reference parse tree that has the same span and label. That is:\n",
    "\n",
    "- has the same starting point\n",
    "- has the same ending point\n",
    "- has the same non-terminal symbol"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this information we can compute precision and recall for the parser, as:\n",
    "\n",
    "$$ \\text{recall} = \\frac{\\text{\\# of correct constituents in hypothesis}}{\\text{\\# of constituents in reference}}$$\n",
    "\n",
    "$$ \\text{precision} = \\frac{\\text{\\# of correct constituents in hypothesis}}{\\text{\\# of constituents in hypothesis}}$$\n",
    "\n",
    "The $F_1$ metric is computed the usual way, as:\n",
    "\n",
    "$$F_1 = \\frac{2PR}{P+R}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case for the correct constituent counts we consider labels, we compute _labeled_ precision and recall. Otherwise, metrics are _unlabeled_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. PARSEVAL\n",
    "The evaluation method that implements this is known as **PARSEVAL** [(Black et al., 1991)](https://aclanthology.org/H91-1060/).\n",
    "\n",
    "The metric includes an algorithm from _caninicalization_ that removes grammar-specific information and allows comparing parsers with different grammars.\n",
    "\n",
    "The \"canonical\" implementation of **PARSEVAL** is known as [**EVALB**](https://nlp.cs.nyu.edu/evalb/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since constitents are essentially sub-trees of a sentence parse tree, while having the same label and start and end points, they might differ in their internal structure. \n",
    "\n",
    "e.g.: `((A B) C)` vs. `(A (B C))`\n",
    "\n",
    "Consequently, there is an additional metric in **PARSEVAL** that is used to account for this -- **cross-brackets** (since parse trees are represented using bracketed notation) -- which is a simple count of such cases per sentence.\n",
    "\n",
    "However, since `((A B) C)` and `(A (B C))` have different constituents, this is already reflected in precision and recall. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. EVALB (PYTHON)\n",
    "There is no officially supported EVALB for python. However, there are few implementations available.\n",
    "The most easy to use it [PYEVALB](https://github.com/flyaway1217/PYEVALB).\n",
    "\n",
    "It is possible to compare two parse trees as the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PYEVALB\n",
      "  Downloading PYEVALB-0.1.3-py3-none-any.whl (13 kB)\n",
      "Collecting pytablewriter>=0.10.2\n",
      "  Downloading pytablewriter-0.64.2-py3-none-any.whl (106 kB)\n",
      "     -------------------------------------- 106.6/106.6 kB 3.0 MB/s eta 0:00:00\n",
      "Collecting tabledata<2,>=1.3.0\n",
      "  Downloading tabledata-1.3.1-py3-none-any.whl (11 kB)\n",
      "Collecting tcolorpy<1,>=0.0.5\n",
      "  Downloading tcolorpy-0.1.2-py3-none-any.whl (7.9 kB)\n",
      "Collecting mbstrdecoder<2,>=1.0.0\n",
      "  Downloading mbstrdecoder-1.1.2-py3-none-any.whl (7.7 kB)\n",
      "Collecting typepy[datetime]<2,>=1.2.0\n",
      "  Downloading typepy-1.3.0-py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: setuptools>=38.3.0 in c:\\users\\maxst\\miniconda3\\envs\\nlu\\lib\\site-packages (from pytablewriter>=0.10.2->PYEVALB) (65.6.3)\n",
      "Collecting pathvalidate<3,>=2.3.0\n",
      "  Downloading pathvalidate-2.5.2-py3-none-any.whl (20 kB)\n",
      "Collecting DataProperty<2,>=0.55.0\n",
      "  Downloading DataProperty-0.55.0-py3-none-any.whl (26 kB)\n",
      "Collecting chardet<6,>=3.0.4\n",
      "  Downloading chardet-5.1.0-py3-none-any.whl (199 kB)\n",
      "     -------------------------------------- 199.1/199.1 kB 4.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in c:\\users\\maxst\\miniconda3\\envs\\nlu\\lib\\site-packages (from typepy[datetime]<2,>=1.2.0->pytablewriter>=0.10.2->PYEVALB) (23.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.0 in c:\\users\\maxst\\miniconda3\\envs\\nlu\\lib\\site-packages (from typepy[datetime]<2,>=1.2.0->pytablewriter>=0.10.2->PYEVALB) (2.8.2)\n",
      "Collecting pytz>=2018.9\n",
      "  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "     -------------------------------------- 502.3/502.3 kB 3.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\maxst\\miniconda3\\envs\\nlu\\lib\\site-packages (from python-dateutil<3.0.0,>=2.8.0->typepy[datetime]<2,>=1.2.0->pytablewriter>=0.10.2->PYEVALB) (1.16.0)\n",
      "Installing collected packages: pytz, tcolorpy, pathvalidate, chardet, mbstrdecoder, typepy, DataProperty, tabledata, pytablewriter, PYEVALB\n",
      "Successfully installed DataProperty-0.55.0 PYEVALB-0.1.3 chardet-5.1.0 mbstrdecoder-1.1.2 pathvalidate-2.5.2 pytablewriter-0.64.2 pytz-2023.3 tabledata-1.3.1 tcolorpy-0.1.2 typepy-1.3.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install PYEVALB\n",
    "from PYEVALB import scorer as eval_scorer\n",
    "from PYEVALB import parser as eval_parser\n",
    "\n",
    "pt0 = \"(S (NP (PRON I)) (VP (V saw) (NP (Det the) (N man)) (PP (P with) (NP (Det a) (N telescope)))))\"\n",
    "pt1 = \"(S (NP (PRON I)) (VP (V saw) (NP (Det the) (N man) (PP (P with) (NP (Det a) (N telescope))))))\"\n",
    "\n",
    "pt0_tree = eval_parser.create_from_bracket_string(pt0)\n",
    "pt1_tree = eval_parser.create_from_bracket_string(pt1)\n",
    "\n",
    "s = eval_scorer.Scorer()\n",
    "result = s.score_trees(pt0_tree, pt1_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: 0.83 R: 0.83\n"
     ]
    }
   ],
   "source": [
    "print(\"P: {} R: {}\".format(round(result.prec, 2), round(result.recall, 2)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Exercise\n",
    "\n",
    "Generate 10 sentence by using a PCFG of your choice.\n",
    "- Experiment with depth and start paramenters\n",
    "- Experiment with at least 2 parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.parse.generate import generate\n",
    "\n",
    "weighted_rules = [\n",
    "    'S -> NP VP [1.0]',\n",
    "    'NP -> Det N [0.6]',\n",
    "    'NP -> Det N PP [0.3]',\n",
    "    'NP -> PRON [0.1]',\n",
    "    'VP -> V NP [0.7]',\n",
    "    'VP -> V NP PP [0.3]',\n",
    "    'PP -> P NP [1.0]',\n",
    "    'Det -> \"the\" [0.5]',\n",
    "    'Det -> \"a\" [0.5]',\n",
    "    'N -> \"man\" [0.5]',\n",
    "    'N -> \"telescope\" [0.5]',\n",
    "    'PRON -> \"I\" [1.0]',\n",
    "    'V -> \"saw\" [1.0]',\n",
    "    'P -> \"with\" [1.0]'   \n",
    "]\n",
    "\n",
    "my_grammar = nltk.PCFG.fromstring(weighted_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ViterbiParser ---\n",
      "['the', 'man', 'saw', 'the', 'man']\n",
      "['the', 'man', 'saw', 'the', 'telescope']\n",
      "['the', 'man', 'saw', 'a', 'man']\n",
      "['the', 'man', 'saw', 'a', 'telescope']\n",
      "['the', 'man', 'saw', 'the', 'man', 'with', 'the', 'man']\n",
      "['the', 'man', 'saw', 'the', 'man', 'with', 'the', 'telescope']\n",
      "['the', 'man', 'saw', 'the', 'man', 'with', 'a', 'man']\n",
      "['the', 'man', 'saw', 'the', 'man', 'with', 'a', 'telescope']\n",
      "['the', 'man', 'saw', 'the', 'man', 'with', 'the', 'man', 'with', 'the', 'man']\n",
      "['the', 'man', 'saw', 'the', 'man', 'with', 'the', 'man', 'with', 'the', 'telescope']\n",
      "--- LongestChartParser ---\n",
      "['the', 'man', 'saw', 'the', 'man']\n",
      "['the', 'man', 'saw', 'the', 'telescope']\n",
      "['the', 'man', 'saw', 'a', 'man']\n",
      "['the', 'man', 'saw', 'a', 'telescope']\n",
      "['the', 'man', 'saw', 'the', 'man', 'with', 'the', 'man']\n",
      "['the', 'man', 'saw', 'the', 'man', 'with', 'the', 'telescope']\n",
      "['the', 'man', 'saw', 'the', 'man', 'with', 'a', 'man']\n",
      "['the', 'man', 'saw', 'the', 'man', 'with', 'a', 'telescope']\n",
      "['the', 'man', 'saw', 'the', 'man', 'with', 'the', 'man', 'with', 'the', 'man']\n",
      "['the', 'man', 'saw', 'the', 'man', 'with', 'the', 'man', 'with', 'the', 'telescope']\n",
      "--- RandomChartParser ---\n",
      "['the', 'man', 'saw', 'the', 'man']\n",
      "['the', 'man', 'saw', 'the', 'telescope']\n",
      "['the', 'man', 'saw', 'a', 'man']\n",
      "['the', 'man', 'saw', 'a', 'telescope']\n",
      "['the', 'man', 'saw', 'the', 'man', 'with', 'the', 'man']\n",
      "['the', 'man', 'saw', 'the', 'man', 'with', 'the', 'telescope']\n",
      "['the', 'man', 'saw', 'the', 'man', 'with', 'a', 'man']\n",
      "['the', 'man', 'saw', 'the', 'man', 'with', 'a', 'telescope']\n",
      "['the', 'man', 'saw', 'the', 'man', 'with', 'the', 'man', 'with', 'the', 'man']\n",
      "['the', 'man', 'saw', 'the', 'man', 'with', 'the', 'man', 'with', 'the', 'telescope']\n"
     ]
    }
   ],
   "source": [
    "from nltk.parse.generate import generate\n",
    "parsers = [nltk.ViterbiParser(my_grammar), nltk.LongestChartParser(my_grammar), nltk.RandomChartParser(my_grammar)]\n",
    "for parser in parsers:\n",
    "  print(\"---\", parser.__class__.__name__, \"---\")\n",
    "  for sent in generate(my_grammar, n=10):\n",
    "    print(sent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
