{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sequence Labeling: Shallow Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "- Understanding: \n",
    "    - relation between Sequence Labeling and Shallow Parsing\n",
    "    - IOB Notation\n",
    "    - Joint Segmentation and Classification\n",
    "    - Feature Engineering\n",
    "- Learning how to:\n",
    "    - use Named Entity Recognition in \n",
    "        - spacy\n",
    "        - NLTK\n",
    "    - train, test, and evaluate Conditional Random Fields models\n",
    "    - perform feature engineering with CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recommended Reading\n",
    "- Dan Jurafsky and James H. Martin. [__Speech and Language Processing__ (SLP)](https://web.stanford.edu/~jurafsky/slp3/) (3rd ed. draft)\n",
    "- Steven Bird, Ewan Klein, and Edward Loper. [__Natural Language Processing with Python__ (NLTK)](https://www.nltk.org/book/)\n",
    "- Conditional Random Fields\n",
    "    - Lafferty et al. (2001) [Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.26.803&rep=rep1&type=pdf) (__original paper__)\n",
    "    - Sutton & McCallum's [An Introduction to Conditional Random Fields](https://homepages.inf.ed.ac.uk/csutton/publications/crftutv2.pdf)\n",
    "    - Edwin Chen's [Introduction to Conditional Random Fields](http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/)\n",
    "    - Michael Collin's [Log-Linear Models, MEMMs, and CRFs](http://www.cs.columbia.edu/~mcollins/crf.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covered Material\n",
    "- SLP\n",
    "    - [Chapter 8: Sequence Labeling for Parts of Speech and Named Entities](https://web.stanford.edu/~jurafsky/slp3/8.pdf) \n",
    "- NLTK \n",
    "    - [Chapter 7: Extracting Information from Text](https://www.nltk.org/book/ch07.html) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Requirements\n",
    "\n",
    "- [spaCy](https://spacy.io/)\n",
    "- [NLTK](https://www.nltk.org/)\n",
    "- [`conll.py`](https://github.com/esrel/LUS/) (in `src` folder)\n",
    "- [CRFsuite](http://www.chokkan.org/software/crfsuite/)\n",
    "    - [python-crfsuite](https://python-crfsuite.readthedocs.io) python binding to `CRFsuite`.\n",
    "    - [sklearn-crfsuite](https://sklearn-crfsuite.readthedocs.io) `python-crfsuite` wrapper providing API similar to `scikit-learn`\n",
    "    - you need to install both `python_crfsuite` and `sklearn_crfsuite` to use `sklearn_crfsuite`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Sequence Labeling and Shallow Parsing\n",
    "\n",
    "Below are some examples of NLP tasks that Sequence Labeling is applied to as one of the methods.\n",
    "\n",
    "The scenario when members of a sequence are mapped to higher order units (i.e. grouped together `[['a'],['b','c']]`) and assigned a category) is known as __shallow parsing__.\n",
    "\n",
    "- [Part-of-Speech Tagging](https://en.wikipedia.org/wiki/Part-of-speech_tagging)\n",
    "- [Shallow Parsing](https://en.wikipedia.org/wiki/Shallow_parsing) (Chunking)\n",
    "    - [Phrase Chunking](https://en.wikipedia.org/wiki/Phrase_chunking)\n",
    "    - [Named-Entity Recognition](https://en.wikipedia.org/wiki/Named-entity_recognition) \n",
    "    - [Semantic Role Labeling](https://en.wikipedia.org/wiki/Semantic_role_labeling)\n",
    "    - Dependency [Parsing](https://en.wikipedia.org/wiki/Parsing) \n",
    "    - Discourse Parsing\n",
    "    - (Natural/Spoken) __Language Understanding__: Concept Tagging/Entity Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.1. The General Setting for Sequence Labeling\n",
    "\n",
    "- Create __training__ and __testing__ sets by tagging a certain amount of text by hand\n",
    "    - i.e. map each word in corpus to a tag\n",
    "- Train tagging model to extract generalizations from the annotated __training__ set\n",
    "- Evaluate the trained tagging model on the annotated __testing__ set\n",
    "- Use the trained tagging model too annotate new texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Shallow Parsing\n",
    "\n",
    "As we have already mentioned, [Shallow Parsing](https://en.wikipedia.org/wiki/Shallow_parsing) is a kind of Sequence Labeling. The main difference from Sequence Labeling task, such as Part-of-Speech tagging, where there is an output label (tag) per token; Shallow Parsing additionally performs __chunking__ -- segmentation of input sequence into constituents. Chunking is required to identify categories (or types) of *multi-word expressions*.\n",
    "In other words, we want to be able to capture information that expressions like `\"New York\"` that consist of 2 tokens, constitute a single unit.\n",
    "\n",
    "What this means in practice is that Shallow Parsing performs *jointly* (or not) 2 tasks:\n",
    "- __Segmentation__ of input into constituents (__spans__)\n",
    "- __Classification__ (Categorization, Labeling) of these constituents into predefined set of labels (__types__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Encoding Segmentation Information: CoNLL Corpus Format\n",
    "\n",
    "Corpus in CoNLL format consists of series of sentences, separated by blank lines. Each sentence is encoded using a table (or \"grid\") of values, where each line corresponds to a single word, and each column corresponds to an annotation type. \n",
    "\n",
    "The set of columns used by CoNLL-style files can vary from corpus to corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "        Alex       B-PER\n",
    "        is         O\n",
    "        going      O\n",
    "        to         O\n",
    "        Los        B-LOC\n",
    "        Angeles    I-LOC\n",
    "        in         O\n",
    "        California B-LOC\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.1. [IOB Scheme](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging))\n",
    "\n",
    "- The notation scheme is used to label *multi-word* spans in token-per-line format.\n",
    "    - *Los Angeles* is a *LOCATION* concept that has 2 tokens\n",
    "- Both, prefix and suffix notations are commons: \n",
    "    - prefix: __B-LOC__\n",
    "    - suffix: __LOC-B__\n",
    "\n",
    "- Meaning of Prefixes\n",
    "    - __B__ for (__B__)eginning of span\n",
    "    - __I__ for (__I__)nside of span\n",
    "    - __O__ for (__O__)utside of span (no prefix or suffix, just `O`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Alternative Schemes:\n",
    "- No prefix or suffix (useful when there are no *multi-word* concepts)\n",
    "```\n",
    "        Alex       PER\n",
    "        is         O\n",
    "        going      O\n",
    "        to         O\n",
    "        Los        LOC\n",
    "        Angeles    LOC\n",
    "        in         O\n",
    "        California LOC\n",
    "```\n",
    "- __IOB/IOB2/BIO__\n",
    "\n",
    "- __IOBE__\n",
    "    - IOB + \n",
    "    - __E__ for (__E__)nd of span (or __L__ for (__L__)ast)\n",
    "```\n",
    "        Alex       B-PER\n",
    "        is         O\n",
    "        going      O\n",
    "        to         O\n",
    "        Los        B-LOC\n",
    "        Angeles    E-LOC\n",
    "        in         O\n",
    "        California B-LOC\n",
    "```\n",
    "    \n",
    "- __BILOU/BIOES__\n",
    "    - IOB + \n",
    "    - __L__ for (__L__)ast word of span\n",
    "    - __U__ for (__U__)nit word (or __S__ for (__S__)ingleton)\n",
    "```\n",
    "        Alex       U-PER\n",
    "        is         O\n",
    "        going      O\n",
    "        to         O\n",
    "        Los        B-LOC\n",
    "        Angeles    E-LOC\n",
    "        in         O\n",
    "        California U-LOC\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Choice of Scheme\n",
    "- It is possible to convert IOB, IOBE, & BILOU formats to each other\n",
    "- Each prefix is applied to every concept label, consequently we increase the number of transitions whose probabilities we need to estimate; \n",
    "    - increasing data sparseness, as for each label we will have less observations\n",
    "- The choice of scheme depends on the amount of available data:\n",
    "    - __IOB__ for least amount\n",
    "    - __BILOU__ for the most amount "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Terminology\n",
    "There is no strict naming convention regarding schemes (see alternatives) or how each constituent is termed. \n",
    "Below is the terminology used in this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "        Alex       B-PER\n",
    "        is         O\n",
    "        going      O\n",
    "        to         O\n",
    "        Los        B-LOC\n",
    "        Angeles    I-LOC\n",
    "        in         O\n",
    "        California B-LOC\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Interpretation\n",
    "Segmentation and Labeling data formats encode the following information:\n",
    "- in string (sentence) `\"Alex is going to Los Angeles in California\"`\n",
    "- there are 3 __entities__ (a.k.a. chunks, concepts or slots, depending on NLP task and perspective), that have __types__ (labels)\n",
    "    - `PERSON`\n",
    "    - `LOCATION`\n",
    "    \n",
    "- entity of __type__ `PERSON`: \n",
    "    - has __span__:\n",
    "        - as tokens from `0` for *CoNLL*: `[0:1]`\n",
    "    - has __value__: `\"Alex\"`\n",
    "        - string *covered by* (*on included*) in __span__\n",
    " \n",
    "*CoNLL* format encodes __tokenization__ informations. In other words, how string `\"Alex is going to Los Angeles in California\"` is split into tokens. Since most Sequence Labeling algorithms operate on token level, internally the strings are split into tokens, applying *IOB*-like schemes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "- Wirte the IOB tags for the sentence `\"Steve Jobs established Apple Inc.\"`\n",
    "    - 'Steve Jobs' is PER\n",
    "    - 'Apple Inc' is ORG"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Steve : B-PER\n",
    "Jobs : I-PER\n",
    "established : O\n",
    "Apple : B-ORG\n",
    "Inc. : I-ORG\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Named Entity Recognition with NLTK\n",
    "[NLTK](https://www.nltk.org/api/nltk.tag.html) provides implementations of popular sequence labeling algorithms for Part-of-Speech Tagging (including [HMM](https://www.nltk.org/api/nltk.tag.html#module-nltk.tag.hmm)), that can be used for Sequence Labeling in general. \n",
    "\n",
    "- Loading & working with CoNLL format corpora in NLTK\n",
    "- Tagger training & testing (running)\n",
    "\n",
    "To have a custom tagger that labels input text with our __custom label set__, we need to __train__ it on a corpus annotated with this __custom label set__.\n",
    "\n",
    "Addtionally, NLTK provides [Chunking](http://www.nltk.org/api/nltk.chunk.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.1. NLTK Pre-trained NE Chunker\n",
    "\n",
    "NLTK provides a classifier that has already been trained to recognize named entities, accessed with the function `nltk.ne_chunk()`. If we set the parameter `binary=True`, then named entities are just tagged as `NE`; otherwise, the classifier adds category labels such as `PERSON`, `ORGANIZATION`, and `GPE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\maxst\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\maxst\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n",
      "(S\n",
      "  (PERSON Pierre/NNP)\n",
      "  (ORGANIZATION Vinken/NNP)\n",
      "  ,/,\n",
      "  61/CD\n",
      "  years/NNS\n",
      "  old/JJ\n",
      "  ,/,\n",
      "  will/MD\n",
      "  join/VB\n",
      "  the/DT\n",
      "  board/NN\n",
      "  as/IN\n",
      "  a/DT\n",
      "  nonexecutive/JJ\n",
      "  director/NN\n",
      "  Nov./NNP\n",
      "  29/CD\n",
      "  ./.)\n",
      "(S\n",
      "  (NE Pierre/NNP Vinken/NNP)\n",
      "  ,/,\n",
      "  61/CD\n",
      "  years/NNS\n",
      "  old/JJ\n",
      "  ,/,\n",
      "  will/MD\n",
      "  join/VB\n",
      "  the/DT\n",
      "  board/NN\n",
      "  as/IN\n",
      "  a/DT\n",
      "  nonexecutive/JJ\n",
      "  director/NN\n",
      "  Nov./NNP\n",
      "  29/CD\n",
      "  ./.)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "\n",
    "for s in treebank.tagged_sents():\n",
    "    print(s)\n",
    "    print(nltk.ne_chunk(s))\n",
    "    print(nltk.ne_chunk(s, binary=True))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.2. Training NLTK Taggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2002 to\n",
      "[nltk_data]     C:\\Users\\maxst\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2002 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "nltk.download('conll2002')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35651\n",
      "('LOC', 'PER', 'ORG', 'MISC')\n",
      "['Melbourne', '(', 'Australia', ')', ',', '25', 'may', '(', 'EFE', ')', '.']\n",
      "[('Melbourne', 'NP'), ('(', 'Fpa'), ('Australia', 'NP'), (')', 'Fpt'), (',', 'Fc'), ('25', 'Z'), ('may', 'NC'), ('(', 'Fpa'), ('EFE', 'NC'), (')', 'Fpt'), ('.', 'Fp')]\n",
      "(S\n",
      "  (LOC Melbourne/NP)\n",
      "  (/Fpa\n",
      "  (LOC Australia/NP)\n",
      "  )/Fpt\n",
      "  ,/Fc\n",
      "  25/Z\n",
      "  may/NC\n",
      "  (/Fpa\n",
      "  (ORG EFE/NC)\n",
      "  )/Fpt\n",
      "  ./Fp)\n",
      "[('Melbourne', 'NP', 'B-LOC'), ('(', 'Fpa', 'O'), ('Australia', 'NP', 'B-LOC'), (')', 'Fpt', 'O'), (',', 'Fc', 'O'), ('25', 'Z', 'O'), ('may', 'NC', 'O'), ('(', 'Fpa', 'O'), ('EFE', 'NC', 'B-ORG'), (')', 'Fpt', 'O'), ('.', 'Fp', 'O')]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from nltk.corpus import conll2002\n",
    "\n",
    "print(len(conll2002.tagged_sents()))\n",
    "print(conll2002._chunk_types)\n",
    "print(conll2002.sents('esp.train')[0])\n",
    "print(conll2002.tagged_sents('esp.train')[0])\n",
    "print(conll2002.chunked_sents('esp.train')[0])\n",
    "print(conll2002.iob_sents('esp.train')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Melbourne', 'NP', 'B-LOC'), ('(', 'Fpa', 'O'), ('Australia', 'NP', 'B-LOC'), (')', 'Fpt', 'O'), (',', 'Fc', 'O'), ('25', 'Z', 'O'), ('may', 'NC', 'O'), ('(', 'Fpa', 'O'), ('EFE', 'NC', 'B-ORG'), (')', 'Fpt', 'O'), ('.', 'Fp', 'O')]\n",
      "[('Melbourne', 'B-LOC'), ('(', 'O'), ('Australia', 'B-LOC'), (')', 'O'), (',', 'O'), ('25', 'O'), ('may', 'O'), ('(', 'O'), ('EFE', 'B-ORG'), (')', 'O'), ('.', 'O')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maxst\\miniconda3\\envs\\NLU\\lib\\site-packages\\nltk\\tag\\hmm.py:334: RuntimeWarning: overflow encountered in cast\n",
      "  X[i, j] = self._transitions[si].logprob(self._states[j])\n",
      "c:\\Users\\maxst\\miniconda3\\envs\\NLU\\lib\\site-packages\\nltk\\tag\\hmm.py:336: RuntimeWarning: overflow encountered in cast\n",
      "  O[i, k] = self._output_logprob(si, self._symbols[k])\n",
      "c:\\Users\\maxst\\miniconda3\\envs\\NLU\\lib\\site-packages\\nltk\\tag\\hmm.py:332: RuntimeWarning: overflow encountered in cast\n",
      "  P[i] = self._priors.logprob(si)\n",
      "c:\\Users\\maxst\\miniconda3\\envs\\NLU\\lib\\site-packages\\nltk\\tag\\hmm.py:364: RuntimeWarning: overflow encountered in cast\n",
      "  O[i, k] = self._output_logprob(si, self._symbols[k])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3760\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# training hmm on training data: exactly as above\n",
    "import nltk.tag.hmm as hmm\n",
    "\n",
    "hmm_model = hmm.HiddenMarkovModelTrainer()\n",
    "\n",
    "print(conll2002.iob_sents('esp.train')[0])\n",
    "\n",
    "# let's get only word and iob-tag\n",
    "trn_sents = [[(text, iob) for text, pos, iob in sent] for sent in conll2002.iob_sents('esp.train')]\n",
    "print(trn_sents[0])\n",
    "\n",
    "tst_sents = [[(text, iob) for text, pos, iob in sent] for sent in conll2002.iob_sents('esp.testa')]\n",
    "\n",
    "hmm_ner = hmm_model.train(trn_sents)\n",
    "    \n",
    "# evaluation\n",
    "accuracy = hmm_ner.accuracy(tst_sents)\n",
    "\n",
    "print(\"Accuracy: {:6.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### NLTK Chunk Tagger Note\n",
    "HMM uses only words as input, NLTK also povides trainable MaxEnt Chunker Tagger, which unfortunatelly requires `megam` file. Unfortunatelly, it is very convoluted to install. (http://www.umiacs.umd.edu/~hal/megam/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Segmentation \n",
    "Train a tagger to perform *segmentation* of input sentences into constituents\n",
    "- Strip concept information from output labels (i.e. keep only IOB-prefix)\n",
    "- Train tagger to predict segmentation labels\n",
    "- Evaluate segmentation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Melbourne', 'B'), ('(', 'O'), ('Australia', 'B'), (')', 'O'), (',', 'O'), ('25', 'O'), ('may', 'O'), ('(', 'O'), ('EFE', 'B'), (')', 'O'), ('.', 'O')]\n",
      "Accuracy: 0.3333\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# GOAL: train a Hidden Markov Model (HMM) to perform segmentation of sentences into constituents. The goal is to predict only the IOB-prefixes, not the actual entity types. This means you need to strip the entity information from the labels and keep only the \"B-\", \"I-\", or \"O\" parts.\n",
    "\n",
    "import nltk.tag.hmm as hmm\n",
    "from nltk.corpus import conll2002\n",
    "\n",
    "# Initialize the HMM trainer\n",
    "hmm_model = hmm.HiddenMarkovModelTrainer()\n",
    "\n",
    "# let's get only word and iob-tag\n",
    "trn_sents_seg = [[(text, iob.split('-')[0]) for text, pos, iob in sent] for sent in conll2002.iob_sents('esp.train')]\n",
    "print(trn_sents_seg[0])\n",
    "\n",
    "tst_sents_seg = [[(text, iob.split('-')[0]) for text, pos, iob in sent] for sent in conll2002.iob_sents('esp.testa')]\n",
    "\n",
    "hmm_seg = hmm_model.train(trn_sents_seg)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = hmm_seg.accuracy(tst_sents)\n",
    "\n",
    "print(\"Accuracy: {:6.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### CoNLL Eval\n",
    "CoNLL Community developed a perl script to evaluate *segmentation* and *labeling* performance jointly using IOB information. Such evaluation provides more accurate assessment of the shallow parsing performance, in comparison to token-level metrics (e.g. NLTK accuracy).\n",
    "\n",
    "- import `evaluate` function from `conll.py` (example shown)\n",
    "- evaluate tagger predictions\n",
    "- compare performances to token-level accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('La', 'B-LOC'), ('Coruña', 'I-LOC'), (',', 'O'), ('23', 'O'), ('may', 'O'), ('(', 'O'), ('EFECOM', 'B-ORG'), (')', 'O'), ('.', 'O')]\n",
      "[('La', 'B-LOC'), ('Coruña', 'I-LOC'), (',', 'O'), ('23', 'O'), ('may', 'O'), ('(', 'O'), ('EFECOM', 'B-ORG'), (')', 'O'), ('.', 'O')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>f</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PER</th>\n",
       "      <td>0.691</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.399</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORG</th>\n",
       "      <td>0.795</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.528</td>\n",
       "      <td>1400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MISC</th>\n",
       "      <td>0.570</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.299</td>\n",
       "      <td>340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOC</th>\n",
       "      <td>0.030</td>\n",
       "      <td>0.849</td>\n",
       "      <td>0.058</td>\n",
       "      <td>1084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>0.055</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.098</td>\n",
       "      <td>3559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           p      r      f     s\n",
       "PER    0.691  0.280  0.399   735\n",
       "ORG    0.795  0.396  0.528  1400\n",
       "MISC   0.570  0.203  0.299   340\n",
       "LOC    0.030  0.849  0.058  1084\n",
       "total  0.055  0.491  0.098  3559"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# to import conll\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('../src/'))\n",
    "\n",
    "from conll import evaluate\n",
    "# for nice tables\n",
    "import pandas as pd\n",
    "\n",
    "# getting references (note that it is testb this time)\n",
    "refs = [[(text, iob) for text, pos, iob in sent] for sent in conll2002.iob_sents('esp.testb')]\n",
    "print(refs[0])\n",
    "# getting hypotheses\n",
    "hyps = [hmm_ner.tag(s) for s in conll2002.sents('esp.testb')]\n",
    "print(hyps[0])\n",
    "\n",
    "results = evaluate(refs, hyps)\n",
    "\n",
    "pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
    "pd_tbl.round(decimals=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5. Named Entity Recognition with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maxst\\miniconda3\\envs\\NLU\\lib\\site-packages\\spacy\\util.py:762: UserWarning: [W095] Model 'en_core_web_sm' (3.5.0) was trained with spaCy v3.5 and may not be 100% compatible with the current version (3.1.7). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "RegistryError",
     "evalue": "[E893] Could not find function 'spacy.Tagger.v2' in function registry 'architectures'. If you're using a custom function, make sure the code is available. If the function is provided by a third-party package, e.g. spacy-transformers, make sure the package is installed in your environment.\n\nAvailable names: spacy-legacy.CharacterEmbed.v1, spacy-legacy.EntityLinker.v1, spacy-legacy.HashEmbedCNN.v1, spacy-legacy.MaxoutWindowEncoder.v1, spacy-legacy.MishWindowEncoder.v1, spacy-legacy.MultiHashEmbed.v1, spacy-legacy.Tagger.v1, spacy-legacy.TextCatBOW.v1, spacy-legacy.TextCatCNN.v1, spacy-legacy.TextCatEnsemble.v1, spacy-legacy.Tok2Vec.v1, spacy-legacy.TransitionBasedParser.v1, spacy.CharacterEmbed.v2, spacy.EntityLinker.v1, spacy.HashEmbedCNN.v2, spacy.MaxoutWindowEncoder.v2, spacy.MishWindowEncoder.v2, spacy.MultiHashEmbed.v2, spacy.PretrainCharacters.v1, spacy.PretrainVectors.v1, spacy.SpanCategorizer.v1, spacy.Tagger.v1, spacy.TextCatBOW.v2, spacy.TextCatCNN.v2, spacy.TextCatEnsemble.v2, spacy.TextCatLowData.v1, spacy.Tok2Vec.v2, spacy.Tok2VecListener.v1, spacy.TorchBiLSTMEncoder.v1, spacy.TransitionBasedParser.v2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRegistryError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mspacy\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39men_core_web_sm\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m nlp \u001b[39m=\u001b[39m en_core_web_sm\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m      5\u001b[0m txt \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mPierre Vinken, 61 years old, will join the board as a nonexecutive director Nov. 29.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      6\u001b[0m doc \u001b[39m=\u001b[39m nlp(txt)\n",
      "File \u001b[1;32mc:\\Users\\maxst\\miniconda3\\envs\\NLU\\lib\\site-packages\\en_core_web_sm\\__init__.py:10\u001b[0m, in \u001b[0;36mload\u001b[1;34m(**overrides)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39moverrides):\n\u001b[1;32m---> 10\u001b[0m     \u001b[39mreturn\u001b[39;00m load_model_from_init_py(\u001b[39m__file__\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moverrides)\n",
      "File \u001b[1;32mc:\\Users\\maxst\\miniconda3\\envs\\NLU\\lib\\site-packages\\spacy\\util.py:544\u001b[0m, in \u001b[0;36mload_model_from_init_py\u001b[1;34m(init_file, vocab, disable, exclude, config)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model_path\u001b[39m.\u001b[39mexists():\n\u001b[0;32m    543\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE052\u001b[39m.\u001b[39mformat(path\u001b[39m=\u001b[39mdata_path))\n\u001b[1;32m--> 544\u001b[0m \u001b[39mreturn\u001b[39;00m load_model_from_path(\n\u001b[0;32m    545\u001b[0m     data_path,\n\u001b[0;32m    546\u001b[0m     vocab\u001b[39m=\u001b[39;49mvocab,\n\u001b[0;32m    547\u001b[0m     meta\u001b[39m=\u001b[39;49mmeta,\n\u001b[0;32m    548\u001b[0m     disable\u001b[39m=\u001b[39;49mdisable,\n\u001b[0;32m    549\u001b[0m     exclude\u001b[39m=\u001b[39;49mexclude,\n\u001b[0;32m    550\u001b[0m     config\u001b[39m=\u001b[39;49mconfig,\n\u001b[0;32m    551\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\maxst\\miniconda3\\envs\\NLU\\lib\\site-packages\\spacy\\util.py:417\u001b[0m, in \u001b[0;36mload_model_from_path\u001b[1;34m(model_path, meta, vocab, disable, exclude, config)\u001b[0m\n\u001b[0;32m    415\u001b[0m overrides \u001b[39m=\u001b[39m dict_to_dot(config)\n\u001b[0;32m    416\u001b[0m config \u001b[39m=\u001b[39m load_config(config_path, overrides\u001b[39m=\u001b[39moverrides)\n\u001b[1;32m--> 417\u001b[0m nlp \u001b[39m=\u001b[39m load_model_from_config(config, vocab\u001b[39m=\u001b[39;49mvocab, disable\u001b[39m=\u001b[39;49mdisable, exclude\u001b[39m=\u001b[39;49mexclude)\n\u001b[0;32m    418\u001b[0m \u001b[39mreturn\u001b[39;00m nlp\u001b[39m.\u001b[39mfrom_disk(model_path, exclude\u001b[39m=\u001b[39mexclude, overrides\u001b[39m=\u001b[39moverrides)\n",
      "File \u001b[1;32mc:\\Users\\maxst\\miniconda3\\envs\\NLU\\lib\\site-packages\\spacy\\util.py:454\u001b[0m, in \u001b[0;36mload_model_from_config\u001b[1;34m(config, vocab, disable, exclude, auto_fill, validate)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[39m# This will automatically handle all codes registered via the languages\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[39m# registry, including custom subclasses provided via entry points\u001b[39;00m\n\u001b[0;32m    453\u001b[0m lang_cls \u001b[39m=\u001b[39m get_lang_class(nlp_config[\u001b[39m\"\u001b[39m\u001b[39mlang\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m--> 454\u001b[0m nlp \u001b[39m=\u001b[39m lang_cls\u001b[39m.\u001b[39;49mfrom_config(\n\u001b[0;32m    455\u001b[0m     config,\n\u001b[0;32m    456\u001b[0m     vocab\u001b[39m=\u001b[39;49mvocab,\n\u001b[0;32m    457\u001b[0m     disable\u001b[39m=\u001b[39;49mdisable,\n\u001b[0;32m    458\u001b[0m     exclude\u001b[39m=\u001b[39;49mexclude,\n\u001b[0;32m    459\u001b[0m     auto_fill\u001b[39m=\u001b[39;49mauto_fill,\n\u001b[0;32m    460\u001b[0m     validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[0;32m    461\u001b[0m )\n\u001b[0;32m    462\u001b[0m \u001b[39mreturn\u001b[39;00m nlp\n",
      "File \u001b[1;32mc:\\Users\\maxst\\miniconda3\\envs\\NLU\\lib\\site-packages\\spacy\\language.py:1763\u001b[0m, in \u001b[0;36mLanguage.from_config\u001b[1;34m(cls, config, vocab, disable, exclude, meta, auto_fill, validate)\u001b[0m\n\u001b[0;32m   1760\u001b[0m     factory \u001b[39m=\u001b[39m pipe_cfg\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mfactory\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1761\u001b[0m     \u001b[39m# The pipe name (key in the config) here is the unique name\u001b[39;00m\n\u001b[0;32m   1762\u001b[0m     \u001b[39m# of the component, not necessarily the factory\u001b[39;00m\n\u001b[1;32m-> 1763\u001b[0m     nlp\u001b[39m.\u001b[39;49madd_pipe(\n\u001b[0;32m   1764\u001b[0m         factory,\n\u001b[0;32m   1765\u001b[0m         name\u001b[39m=\u001b[39;49mpipe_name,\n\u001b[0;32m   1766\u001b[0m         config\u001b[39m=\u001b[39;49mpipe_cfg,\n\u001b[0;32m   1767\u001b[0m         validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[0;32m   1768\u001b[0m         raw_config\u001b[39m=\u001b[39;49mraw_config,\n\u001b[0;32m   1769\u001b[0m     )\n\u001b[0;32m   1770\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1771\u001b[0m     \u001b[39m# We need the sourced components to reference the same\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m     \u001b[39m# vocab without modifying the current vocab state **AND**\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1777\u001b[0m     \u001b[39m# during deserialization, so they do not need any\u001b[39;00m\n\u001b[0;32m   1778\u001b[0m     \u001b[39m# additional handling.\u001b[39;00m\n\u001b[0;32m   1779\u001b[0m     \u001b[39mif\u001b[39;00m vocab_b \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\maxst\\miniconda3\\envs\\NLU\\lib\\site-packages\\spacy\\language.py:787\u001b[0m, in \u001b[0;36mLanguage.add_pipe\u001b[1;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[0;32m    779\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_factory(factory_name):\n\u001b[0;32m    780\u001b[0m         err \u001b[39m=\u001b[39m Errors\u001b[39m.\u001b[39mE002\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    781\u001b[0m             name\u001b[39m=\u001b[39mfactory_name,\n\u001b[0;32m    782\u001b[0m             opts\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfactory_names),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    785\u001b[0m             lang_code\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlang,\n\u001b[0;32m    786\u001b[0m         )\n\u001b[1;32m--> 787\u001b[0m     pipe_component \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_pipe(\n\u001b[0;32m    788\u001b[0m         factory_name,\n\u001b[0;32m    789\u001b[0m         name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m    790\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[0;32m    791\u001b[0m         raw_config\u001b[39m=\u001b[39;49mraw_config,\n\u001b[0;32m    792\u001b[0m         validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[0;32m    793\u001b[0m     )\n\u001b[0;32m    794\u001b[0m pipe_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_pipe_index(before, after, first, last)\n\u001b[0;32m    795\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pipe_meta[name] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_factory_meta(factory_name)\n",
      "File \u001b[1;32mc:\\Users\\maxst\\miniconda3\\envs\\NLU\\lib\\site-packages\\spacy\\language.py:670\u001b[0m, in \u001b[0;36mLanguage.create_pipe\u001b[1;34m(self, factory_name, name, config, raw_config, validate)\u001b[0m\n\u001b[0;32m    667\u001b[0m cfg \u001b[39m=\u001b[39m {factory_name: config}\n\u001b[0;32m    668\u001b[0m \u001b[39m# We're calling the internal _fill here to avoid constructing the\u001b[39;00m\n\u001b[0;32m    669\u001b[0m \u001b[39m# registered functions twice\u001b[39;00m\n\u001b[1;32m--> 670\u001b[0m resolved \u001b[39m=\u001b[39m registry\u001b[39m.\u001b[39;49mresolve(cfg, validate\u001b[39m=\u001b[39;49mvalidate)\n\u001b[0;32m    671\u001b[0m filled \u001b[39m=\u001b[39m registry\u001b[39m.\u001b[39mfill({\u001b[39m\"\u001b[39m\u001b[39mcfg\u001b[39m\u001b[39m\"\u001b[39m: cfg[factory_name]}, validate\u001b[39m=\u001b[39mvalidate)[\u001b[39m\"\u001b[39m\u001b[39mcfg\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    672\u001b[0m filled \u001b[39m=\u001b[39m Config(filled)\n",
      "File \u001b[1;32mc:\\Users\\maxst\\miniconda3\\envs\\NLU\\lib\\site-packages\\thinc\\config.py:746\u001b[0m, in \u001b[0;36mregistry.resolve\u001b[1;34m(cls, config, schema, overrides, validate)\u001b[0m\n\u001b[0;32m    737\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    738\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mresolve\u001b[39m(\n\u001b[0;32m    739\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    744\u001b[0m     validate: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    745\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Any]:\n\u001b[1;32m--> 746\u001b[0m     resolved, _ \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_make(\n\u001b[0;32m    747\u001b[0m         config, schema\u001b[39m=\u001b[39;49mschema, overrides\u001b[39m=\u001b[39;49moverrides, validate\u001b[39m=\u001b[39;49mvalidate, resolve\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m    748\u001b[0m     )\n\u001b[0;32m    749\u001b[0m     \u001b[39mreturn\u001b[39;00m resolved\n",
      "File \u001b[1;32mc:\\Users\\maxst\\miniconda3\\envs\\NLU\\lib\\site-packages\\thinc\\config.py:795\u001b[0m, in \u001b[0;36mregistry._make\u001b[1;34m(cls, config, schema, overrides, resolve, validate)\u001b[0m\n\u001b[0;32m    793\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_interpolated:\n\u001b[0;32m    794\u001b[0m     config \u001b[39m=\u001b[39m Config(orig_config)\u001b[39m.\u001b[39minterpolate()\n\u001b[1;32m--> 795\u001b[0m filled, _, resolved \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_fill(\n\u001b[0;32m    796\u001b[0m     config, schema, validate\u001b[39m=\u001b[39;49mvalidate, overrides\u001b[39m=\u001b[39;49moverrides, resolve\u001b[39m=\u001b[39;49mresolve\n\u001b[0;32m    797\u001b[0m )\n\u001b[0;32m    798\u001b[0m filled \u001b[39m=\u001b[39m Config(filled, section_order\u001b[39m=\u001b[39msection_order)\n\u001b[0;32m    799\u001b[0m \u001b[39m# Check that overrides didn't include invalid properties not in config\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\maxst\\miniconda3\\envs\\NLU\\lib\\site-packages\\thinc\\config.py:850\u001b[0m, in \u001b[0;36mregistry._fill\u001b[1;34m(cls, config, schema, validate, resolve, parent, overrides)\u001b[0m\n\u001b[0;32m    848\u001b[0m     schema\u001b[39m.\u001b[39m__fields__[key] \u001b[39m=\u001b[39m copy_model_field(field, Any)\n\u001b[0;32m    849\u001b[0m promise_schema \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mmake_promise_schema(value, resolve\u001b[39m=\u001b[39mresolve)\n\u001b[1;32m--> 850\u001b[0m filled[key], validation[v_key], final[key] \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_fill(\n\u001b[0;32m    851\u001b[0m     value,\n\u001b[0;32m    852\u001b[0m     promise_schema,\n\u001b[0;32m    853\u001b[0m     validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[0;32m    854\u001b[0m     resolve\u001b[39m=\u001b[39;49mresolve,\n\u001b[0;32m    855\u001b[0m     parent\u001b[39m=\u001b[39;49mkey_parent,\n\u001b[0;32m    856\u001b[0m     overrides\u001b[39m=\u001b[39;49moverrides,\n\u001b[0;32m    857\u001b[0m )\n\u001b[0;32m    858\u001b[0m reg_name, func_name \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mget_constructor(final[key])\n\u001b[0;32m    859\u001b[0m args, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mparse_args(final[key])\n",
      "File \u001b[1;32mc:\\Users\\maxst\\miniconda3\\envs\\NLU\\lib\\site-packages\\thinc\\config.py:849\u001b[0m, in \u001b[0;36mregistry._fill\u001b[1;34m(cls, config, schema, validate, resolve, parent, overrides)\u001b[0m\n\u001b[0;32m    847\u001b[0m     field \u001b[39m=\u001b[39m schema\u001b[39m.\u001b[39m__fields__[key]\n\u001b[0;32m    848\u001b[0m     schema\u001b[39m.\u001b[39m__fields__[key] \u001b[39m=\u001b[39m copy_model_field(field, Any)\n\u001b[1;32m--> 849\u001b[0m promise_schema \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mmake_promise_schema(value, resolve\u001b[39m=\u001b[39;49mresolve)\n\u001b[0;32m    850\u001b[0m filled[key], validation[v_key], final[key] \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_fill(\n\u001b[0;32m    851\u001b[0m     value,\n\u001b[0;32m    852\u001b[0m     promise_schema,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    856\u001b[0m     overrides\u001b[39m=\u001b[39moverrides,\n\u001b[0;32m    857\u001b[0m )\n\u001b[0;32m    858\u001b[0m reg_name, func_name \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mget_constructor(final[key])\n",
      "File \u001b[1;32mc:\\Users\\maxst\\miniconda3\\envs\\NLU\\lib\\site-packages\\thinc\\config.py:1040\u001b[0m, in \u001b[0;36mregistry.make_promise_schema\u001b[1;34m(cls, obj, resolve)\u001b[0m\n\u001b[0;32m   1038\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m resolve \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mhas(reg_name, func_name):\n\u001b[0;32m   1039\u001b[0m     \u001b[39mreturn\u001b[39;00m EmptySchema\n\u001b[1;32m-> 1040\u001b[0m func \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mget(reg_name, func_name)\n\u001b[0;32m   1041\u001b[0m \u001b[39m# Read the argument annotations and defaults from the function signature\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m id_keys \u001b[39m=\u001b[39m [k \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m obj\u001b[39m.\u001b[39mkeys() \u001b[39mif\u001b[39;00m k\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m@\u001b[39m\u001b[39m\"\u001b[39m)]\n",
      "File \u001b[1;32mc:\\Users\\maxst\\miniconda3\\envs\\NLU\\lib\\site-packages\\spacy\\util.py:138\u001b[0m, in \u001b[0;36mregistry.get\u001b[1;34m(cls, registry_name, func_name)\u001b[0m\n\u001b[0;32m    136\u001b[0m             \u001b[39mpass\u001b[39;00m\n\u001b[0;32m    137\u001b[0m     available \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39msorted\u001b[39m(reg\u001b[39m.\u001b[39mget_all()\u001b[39m.\u001b[39mkeys())) \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 138\u001b[0m     \u001b[39mraise\u001b[39;00m RegistryError(\n\u001b[0;32m    139\u001b[0m         Errors\u001b[39m.\u001b[39mE893\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    140\u001b[0m             name\u001b[39m=\u001b[39mfunc_name, reg_name\u001b[39m=\u001b[39mregistry_name, available\u001b[39m=\u001b[39mavailable\n\u001b[0;32m    141\u001b[0m         )\n\u001b[0;32m    142\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    143\u001b[0m \u001b[39mreturn\u001b[39;00m func\n",
      "\u001b[1;31mRegistryError\u001b[0m: [E893] Could not find function 'spacy.Tagger.v2' in function registry 'architectures'. If you're using a custom function, make sure the code is available. If the function is provided by a third-party package, e.g. spacy-transformers, make sure the package is installed in your environment.\n\nAvailable names: spacy-legacy.CharacterEmbed.v1, spacy-legacy.EntityLinker.v1, spacy-legacy.HashEmbedCNN.v1, spacy-legacy.MaxoutWindowEncoder.v1, spacy-legacy.MishWindowEncoder.v1, spacy-legacy.MultiHashEmbed.v1, spacy-legacy.Tagger.v1, spacy-legacy.TextCatBOW.v1, spacy-legacy.TextCatCNN.v1, spacy-legacy.TextCatEnsemble.v1, spacy-legacy.Tok2Vec.v1, spacy-legacy.TransitionBasedParser.v1, spacy.CharacterEmbed.v2, spacy.EntityLinker.v1, spacy.HashEmbedCNN.v2, spacy.MaxoutWindowEncoder.v2, spacy.MishWindowEncoder.v2, spacy.MultiHashEmbed.v2, spacy.PretrainCharacters.v1, spacy.PretrainVectors.v1, spacy.SpanCategorizer.v1, spacy.Tagger.v1, spacy.TextCatBOW.v2, spacy.TextCatCNN.v2, spacy.TextCatEnsemble.v2, spacy.TextCatLowData.v1, spacy.Tok2Vec.v2, spacy.Tok2VecListener.v1, spacy.TorchBiLSTMEncoder.v1, spacy.TransitionBasedParser.v2"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "txt = 'Pierre Vinken, 61 years old, will join the board as a nonexecutive director Nov. 29.'\n",
    "doc = nlp(txt)\n",
    "\n",
    "print([ent.text for ent in doc.ents])\n",
    "print([(t.ent_type_, t.ent_iob_) for t in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Shallow Parsing with Conditional Random Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CRFs](https://en.wikipedia.org/wiki/Conditional_random_field) are a type of __discriminative undirected probabilistic graphical model__. \n",
    "It is a generalization of __any__ undirected graph structure.\n",
    "In Natural Language Processing, the structure is a *sequence* of words, and conditioning is on *previous transition*. This is known as __Linear Chain CRFs__.\n",
    "\n",
    "For general graphs, the problem of exact inference in CRFs is intractable. For __Linear Chain CRFs__, however, there is an exact solution, and the used algorithm is \"analogous to the forward-backward and Viterbi algorithm for the case of Hidden Markov Models\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python CRF Tutorials\n",
    "\n",
    "Authors of the python packages provide tutorials in the form of notebooks already!\n",
    "\n",
    "__Follow the tutorials to learn the tools.__\n",
    "\n",
    "- [sklearn-crfsuite notebook](https://github.com/TeamHG-Memex/sklearn-crfsuite/blob/master/docs/CoNLL2002.ipynb)\n",
    "- [python-crfsuite notebook](https://github.com/scrapinghub/python-crfsuite/blob/master/examples/CoNLL%202002.ipynb)\n",
    "\n",
    "- `bias` is explained [here](https://github.com/scrapinghub/python-crfsuite/issues/73)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare a CRF baseline for our dataset that:\n",
    "- considers only word itself and the previous tag (similar to HMM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8323\n",
      "1915\n",
      "[('Melbourne', 'B-LOC'), ('(', 'O'), ('Australia', 'B-LOC'), (')', 'O'), (',', 'O'), ('25', 'O'), ('may', 'O'), ('(', 'O'), ('EFE', 'B-ORG'), (')', 'O'), ('.', 'O')]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# data set\n",
    "print(len(trn_sents))\n",
    "print(len(tst_sents))\n",
    "print(trn_sents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's copy & re-define feature extraction functions from the tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    return {'bias': 1.0, 'word.lower()': word.lower()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect our baseline features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B-LOC'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "sent2labels(trn_sents[0])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 188 ms\n",
      "Wall time: 258 ms\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trn_feats = [sent2features(s) for s in trn_sents]\n",
    "trn_label = [sent2labels(s) for s in trn_sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn_crfsuite import CRF\n",
    "\n",
    "crf = CRF(\n",
    "    algorithm='lbfgs', \n",
    "    c1=0.1, \n",
    "    c2=0.1, \n",
    "    max_iterations=100, \n",
    "    all_possible_transitions=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 10.7 s\n",
      "Wall time: 12.2 s\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# workaround for scikit-learn 1.0\n",
    "try:\n",
    "    crf.fit(trn_feats, trn_label)\n",
    "except AttributeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "tst_feats = [sent2features(s) for s in tst_sents]\n",
    "pred = crf.predict(tst_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use our `conll` evaluation script. (Notice that tools report token level metrics.)\n",
    "\n",
    "For that we will need to modify our prediction output a bit, to make it a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-LOC', 'I-LOC', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O']\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "hyp = [[(tst_feats[i][j], t) for j, t in enumerate(tokens)] for i, tokens in enumerate(pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>f</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LOC</th>\n",
       "      <td>0.699</td>\n",
       "      <td>0.713</td>\n",
       "      <td>0.706</td>\n",
       "      <td>985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PER</th>\n",
       "      <td>0.859</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.522</td>\n",
       "      <td>1222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORG</th>\n",
       "      <td>0.729</td>\n",
       "      <td>0.442</td>\n",
       "      <td>0.551</td>\n",
       "      <td>1700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MISC</th>\n",
       "      <td>0.542</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.352</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>0.729</td>\n",
       "      <td>0.466</td>\n",
       "      <td>0.569</td>\n",
       "      <td>4352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           p      r      f     s\n",
       "LOC    0.699  0.713  0.706   985\n",
       "PER    0.859  0.375  0.522  1222\n",
       "ORG    0.729  0.442  0.551  1700\n",
       "MISC   0.542  0.261  0.352   445\n",
       "total  0.729  0.466  0.569  4352"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "results = evaluate(tst_sents, hyp)\n",
    "\n",
    "pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
    "pd_tbl.round(decimals=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the strengths of CRFs lies in its ability to make use of rich feature representation. The process of extracting features from raw data is know as [feature engineering](https://en.wikipedia.org/wiki/Feature_engineering).\n",
    "\n",
    "Common features used in sequence labeling with CRFs are:\n",
    "- part-of-speech tags\n",
    "- lemmas\n",
    "- token character prefixes and suffixes (e.g. first and last 1, 2, 3 characters of a word; `word[-3:]` in tutorial is suffix of length 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. SpaCy Token Features\n",
    "\n",
    "[spaCy](https://spacy.io/) provides a convenient way to augment our feature set with common features using in Natural Language Processing. \n",
    "\n",
    "The list of provided token-level features is available [here](https://spacy.io/api/token#attributes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Adding Features to CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's modify `sent2features` function to make use of spaCy features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import es_core_news_sm\n",
    "nlp = es_core_news_sm.load()\n",
    "\n",
    "# nlp = spacy.load(\"es_core_news_sm\")\n",
    "nlp.tokenizer = Tokenizer(nlp.vocab)  # to use white space tokenization (generally a bad idea for unknown data)\n",
    "\n",
    "def sent2spacy_features(sent):\n",
    "    spacy_sent = nlp(\" \".join(sent2tokens(sent)))\n",
    "    feats = []\n",
    "    for token in spacy_sent:\n",
    "        token_feats = {\n",
    "            'bias': 1.0,\n",
    "            'word.lower()': token.lower_,\n",
    "            'pos': token.pos_,\n",
    "            'lemma': token.lemma_\n",
    "        }\n",
    "        feats.append(token_feats)\n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sent2tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trn_feats \u001b[39m=\u001b[39m [sent2spacy_features(s) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m trn_sents]\n\u001b[0;32m      2\u001b[0m trn_label \u001b[39m=\u001b[39m [sent2labels(s) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m trn_sents]\n\u001b[0;32m      3\u001b[0m tst_feats \u001b[39m=\u001b[39m [sent2spacy_features(s) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m tst_sents]\n",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trn_feats \u001b[39m=\u001b[39m [sent2spacy_features(s) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m trn_sents]\n\u001b[0;32m      2\u001b[0m trn_label \u001b[39m=\u001b[39m [sent2labels(s) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m trn_sents]\n\u001b[0;32m      3\u001b[0m tst_feats \u001b[39m=\u001b[39m [sent2spacy_features(s) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m tst_sents]\n",
      "Cell \u001b[1;32mIn[9], line 10\u001b[0m, in \u001b[0;36msent2spacy_features\u001b[1;34m(sent)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msent2spacy_features\u001b[39m(sent):\n\u001b[1;32m---> 10\u001b[0m     spacy_sent \u001b[39m=\u001b[39m nlp(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(sent2tokens(sent)))\n\u001b[0;32m     11\u001b[0m     feats \u001b[39m=\u001b[39m []\n\u001b[0;32m     12\u001b[0m     \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m spacy_sent:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sent2tokens' is not defined"
     ]
    }
   ],
   "source": [
    "trn_feats = [sent2spacy_features(s) for s in trn_sents]\n",
    "trn_label = [sent2labels(s) for s in trn_sents]\n",
    "tst_feats = [sent2spacy_features(s) for s in tst_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trn_feats[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = CRF(\n",
    "    algorithm='lbfgs', \n",
    "    c1=0.1, \n",
    "    c2=0.1, \n",
    "    max_iterations=100, \n",
    "    all_possible_transitions=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# workaround for scikit-learn 1.0\n",
    "try:\n",
    "    crf.fit(trn_feats, trn_label)\n",
    "except AttributeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = crf.predict(tst_feats)\n",
    "\n",
    "hyp = [[(tst_feats[i][j], t) for j, t in enumerate(tokens)] for i, tokens in enumerate(pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate(tst_sents, hyp)\n",
    "\n",
    "pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
    "pd_tbl.round(decimals=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the last lab exercise, as for POS tags, you have to tokenize the sentences by white space. I added the code for white space tokenization in stanza, which is just a flag set to true in the pipeline. \n",
    "\n",
    "Moreover, some of your colleagues had some issues with spacy_conll. This is due to the version of pandas and I can confirm that with  pandas==1.3.5 the library works. Furthermore, depending on the version of spacy/stanza you may have to rename some columns of the data frame. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exsecise is to experiment with a CRF model on NER task. You have to train and test a CRF with using different features on the conll2002 corpus (the same that we used here in the lab). \n",
    "\n",
    "The features that you have to experiment with are:\n",
    "- Baseline using the fetures in sent2spacy_features\n",
    "    - Train the model and print results on the test set\n",
    "- Add the \"suffix\" feature\n",
    "    - Train the model and print results on the test set\n",
    "- Add all the features used in the [tutorial on CoNLL dataset](https://github.com/TeamHG-Memex/sklearn-crfsuite/blob/master/docs/CoNLL2002.ipynb)\n",
    "    - Train the model and print results on the test set\n",
    "- Increase the feature window (number of previous and next token) to:\n",
    "    - `[-1, +1]`\n",
    "        - Train the model and print results on the test set\n",
    "    - `[-2, +2]`\n",
    "        - Train the model and print results on the test set\n",
    "\n",
    "        \n",
    "The format of the results has to be the table that we used so far, that is:\n",
    "```python\n",
    "results = evaluate(tst_sents, hyp)\n",
    "pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
    "pd_tbl.round(decimals=3)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
