# lab 6

UAS=0 for both SPacy and Stanza
followed the baseline code after the explanation of the exercise and used the trebank sentences instead
dependency_treebank doesn't have labels while stanza and spacy have them

# lab 9
üî• Major: initialize the model with the pre-trained weifghts (download from hugging-face and fine-tune)
(part 1) train progressively all models, one for each improvement
üéØ Goal: find good parameters (20 min on azure, 10 min to converge, 100 epochs to get around 184 PPL (eval))
you have to test:
- LSTM + SGD
- LSTM + dropouts + SGD
- LSTM + dropouts + AdamW
‚ö†Ô∏è Warning: PPL should be less than 250 for both 3 versions
üî• Major:
- for each part, multiple tests given a combination of changes
- for each test, do at least 2 runs to have realistic results (and validate them)

üóíÔ∏è Note: from paper "For the specific task of neural language modeling, traditionally SGD without momentum has been found to outperform other algorithms such as momentum SGD, Adam, Adagrad and RMSProp by a statistically significant margin."

‚ùì Problem: using AdamW leads to better results -> why we should implement NT-AvSGD? (Weight Tying and Variational Dropout result in PPL of 100 with AdamW, but using SGD it get stuck around 600
üóíÔ∏è Note: I am using lr of 0.1 for SGD and 0.001 for AdamW), I don't expect that from 600 ppl with NT-AvSGD it can arrive at 100pplüòÖ Did anyone had the same problem?
üí° Idea:
* part 1: I can achieve ~180 PPL with AdamW (100 epochs) with lr 1e-4 and around the same with all SGD variants, lr 1
* part 2: it takes longer for me (more than 100 epochs)

For dropout I have the same, I am using a 0.1 probably, it may be too low and so using it is not so impacting
I‚Äôm using the same probabilities as in the paper: 0.4 embeddings, 0.2 outputs
(part 2) modify the baseline and add some regularisations

# lab 10

play a bit with the parameters (the learning rate, hidden size of the layers and batch size) to maximise the performance

for each test, do at least 2 runs to have realistic results (and validate them)

part 2: I do not undertand if we have to train the bert model from scratch or finetune it
fine tuning the huggingface model I think. The objectivity classification model from part 1 is used for data filtering
But with fine tuning we have the problem that the bert tokenizer can split in more tokens one word, and this will cause problems with the label associated at word label
The difficulty consists in coming up with a masking technique to overcome this. In the paper they mention a simple technique, it took me a function and some hack to embed the masks but it works
The Bert tokenizer has a function "convert_tokens_to_ids", doesn't that solve the issue? We could convert to IDs the whitespace tokenized text
point is that words not in the vocabulary are broken into subtokens. Subtokens become logits. Logits then don‚Äôt match word level labels. You need to go back to the word sequence with the right subtokens.
but it doesn't tokenize further the text. For unseen words it just has an ID referred as UNK (function "convert_tokens_to_ids")

also you can make up a word and feed it to tokenizer.tokenize()
Yes, I know, I am just saying you could use tokenizer.convert_tokens_to_ids instead of tokenizer.tokenize().
I did that and worked for me, but I could be wrong of course

# lab 11

play a bit with the parameters (the learning rate, hidden size of the layers and batch size) to maximise the performance

for each test, do at least 2 runs to have realistic results (and validate them)

# general

train and evaluate multiple times only in labs 10 and 11, not 9