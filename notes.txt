# lab 9
‚ö†Ô∏è Warning: PPL should be less than 250 for both 3 versions
üî• Major:
- for each part, multiple tests given a combination of changes
- for each test, do at least 2 runs to have realistic results (and validate them)

üóíÔ∏è Note: from paper "For the specific task of neural language modeling, traditionally SGD without momentum has been found to outperform other algorithms such as momentum SGD, Adam, Adagrad and RMSProp by a statistically significant margin."

‚ùì Problem: using AdamW leads to better results -> why we should implement NT-AvSGD? (Weight Tying and Variational Dropout result in PPL of 100 with AdamW, but using SGD it get stuck around 600
üóíÔ∏è Note: I am using lr of 0.1 for SGD and 0.001 for AdamW), I don't expect that from 600 ppl with NT-AvSGD it can arrive at 100pplüòÖ Did anyone had the same problem?
üí° Idea:
* part 1: I can achieve ~180 PPL with AdamW (100 epochs) with lr 1e-4 and around the same with all SGD variants, lr 1
* part 2: it takes longer for me (more than 100 epochs)

For dropout I have the same, I am using a 0.1 probably, it may be too low and so using it is not so impacting
I‚Äôm using the same probabilities as in the paper: 0.4 embeddings, 0.2 outputs

# lab 10

‚ùì Problem: the bert tokenizer can split in more tokens one word, and this will cause problems with the label associated at word label
üí° Idea: the difficulty consists in coming up with a masking technique to overcome this.
In the paper they mention a simple technique, it took me a function and some hack to embed the masks but it works
The Bert tokenizer has a function "convert_tokens_to_ids", doesn't that solve the issue? We could convert to IDs the whitespace tokenized text
point is that words not in the vocabulary are broken into subtokens. Subtokens become logits. Logits then don‚Äôt match word level labels. You need to go back to the word sequence with the right subtokens.
but it doesn't tokenize further the text. For unseen words it just has an ID referred as UNK (function "convert_tokens_to_ids")

also you can make up a word and feed it to tokenizer.tokenize()
Yes, I know, I am just saying you could use tokenizer.convert_tokens_to_ids instead of tokenizer.tokenize().
I did that and worked for me, but I could be wrong of course